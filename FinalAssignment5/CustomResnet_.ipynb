{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomResnet .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanyam8055/EIP-4.0-/blob/master/FinalAssignment5/CustomResnet_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzlqBe2uspoW",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6fgLrlQq_xN",
        "colab_type": "code",
        "outputId": "5c81dc70-59b0-4b8f-ad23-544916548720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#!unzip -q \"/content/drive/My Drive/Colab Notebooks/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls\n",
        "%tensorflow_version 1.x\n",
        "!pip install bokeh\n",
        "import cv2\n",
        "import json\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16,InceptionV3, ResNet50V2\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam,SGD,RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "\n",
        "df.head()\n",
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=1)\n",
        "train_df.shape, val_df.shape\n",
        "train_df.head()\n",
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=128)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=128, shuffle=True)\n",
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "from keras import layers\n",
        "from keras.callbacks import *\n",
        "filepath=\"/content/drive/My Drive/Colab Notebooks/check.ckpt\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_loss', verbose=1,save_weights_only=False, save_best_only=True)  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh) (20.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.13)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.6.1)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.11.1)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh) (4.5.3)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh) (6.2.2)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.17.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (2.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4HbjcpasfTf",
        "colab_type": "text"
      },
      "source": [
        "# MODEL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvcJY5GWypBm",
        "colab_type": "code",
        "outputId": "ec1611e2-acc8-4754-b698-943ef7c49ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def residual_block(y, nb_channels, kernel_size, init):\n",
        "\n",
        "    shortcut = y\n",
        "\n",
        "    y = layers.Conv2D(nb_channels, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_initializer=init)(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.ReLU()(y)\n",
        "\n",
        "    y = layers.Conv2D(nb_channels, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_initializer=init)(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    \n",
        "    y = layers.add([shortcut, y])\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.ReLU()(y)\n",
        "    \n",
        "\n",
        "    return y\n",
        "def resnet_7():\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "    img_height = 224\n",
        "    img_width = 224\n",
        "    channel = 3\n",
        "    input = Input(shape=(img_height, img_width, channel,))\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same', kernel_initializer=init, input_shape=(224,224,3))(input)\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(1,1), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=2, strides = (2,2))(x)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(3, 3),strides=(1,1), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = residual_block(x, 64, (3, 3), init)\n",
        "    x = layers.Conv2D(64, kernel_size=(3,3),strides=(1,1), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.MaxPooling2D(pool_size=2, strides = (2,2))(x)\n",
        "    x = layers.Conv2D(128, kernel_size=(3, 3),strides=(1,1), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = residual_block(x, 128, (3, 3), init)\n",
        "    x = layers.Conv2D(128, kernel_size=(3,3),strides=(1,1), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.MaxPooling2D(pool_size=2, strides = (2,2))(x)\n",
        "\n",
        "\n",
        "    \n",
        "    x = layers.Conv2D(256, kernel_size=(3, 3),strides=(1,1), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = residual_block(x, 256, (3, 3), init)\n",
        "    x = layers.Conv2D(256, kernel_size=(3,3),strides=(1,1), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(256, kernel_size=(3,3), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.Conv2D(256, kernel_size=(3,3), strides=1, padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(512, kernel_size=(3,3),padding='same',activation = 'relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=2, strides = (2,2))(x)\n",
        "    x = residual_block(x, 512, (3, 3), init)\n",
        "    x = layers.SeparableConv2D(512, kernel_size=(3,3),strides=1, activation = 'softmax')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    \n",
        "    model = Model(inputs=input, outputs=x)\n",
        "    return model\n",
        "\n",
        "model = resnet_7()\n",
        "  \n",
        "backbone = resnet_7()\n",
        "\n",
        "\n",
        "neck = backbone.output\n",
        "\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(64, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(64, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "from keras.callbacks import *\n",
        "\n",
        "\n",
        "\n",
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 112, 112, 32) 896         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 112, 112, 32) 9248        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 112, 112, 32) 128         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 56, 56, 32)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 56, 56, 64)   18496       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 56, 56, 64)   256         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_12 (ReLU)                 (None, 56, 56, 64)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 56, 56, 64)   36928       re_lu_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 56, 56, 64)   256         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_13 (ReLU)                 (None, 56, 56, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 56, 56, 64)   36928       re_lu_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 56, 56, 64)   256         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 56, 56, 64)   0           re_lu_12[0][0]                   \n",
            "                                                                 batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 56, 56, 64)   256         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_14 (ReLU)                 (None, 56, 56, 64)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 56, 56, 64)   36928       re_lu_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 28, 28, 64)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 28, 28, 128)  73856       max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 28, 28, 128)  512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_15 (ReLU)                 (None, 28, 28, 128)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 28, 28, 128)  147584      re_lu_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 28, 28, 128)  512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_16 (ReLU)                 (None, 28, 28, 128)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 28, 28, 128)  147584      re_lu_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 28, 28, 128)  512         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 128)  0           re_lu_15[0][0]                   \n",
            "                                                                 batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 28, 28, 128)  512         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_17 (ReLU)                 (None, 28, 28, 128)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 28, 28, 128)  147584      re_lu_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 128)  0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 14, 14, 256)  295168      max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 14, 14, 256)  1024        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_18 (ReLU)                 (None, 14, 14, 256)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 14, 14, 256)  590080      re_lu_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 14, 14, 256)  1024        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_19 (ReLU)                 (None, 14, 14, 256)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 14, 14, 256)  590080      re_lu_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 14, 14, 256)  1024        conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 14, 14, 256)  0           re_lu_18[0][0]                   \n",
            "                                                                 batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 14, 14, 256)  1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_20 (ReLU)                 (None, 14, 14, 256)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 14, 14, 256)  590080      re_lu_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 14, 14, 256)  1024        conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 14, 14, 256)  590080      batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 14, 14, 256)  590080      conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 14, 14, 256)  1024        conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 14, 14, 512)  1180160     batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 7, 7, 512)    0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 7, 7, 512)    2359808     max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 7, 7, 512)    2048        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_21 (ReLU)                 (None, 7, 7, 512)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 7, 7, 512)    2359808     re_lu_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 7, 7, 512)    2048        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 7, 7, 512)    0           max_pooling2d_8[0][0]            \n",
            "                                                                 batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 7, 7, 512)    2048        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_22 (ReLU)                 (None, 7, 7, 512)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 5, 5, 512)    267264      re_lu_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 12800)        0           separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 12800)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 12800)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 12800)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 12800)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 12800)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 12800)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 12800)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 12800)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           819264      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64)           819264      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 64)           819264      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 64)           819264      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 64)           819264      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 64)           819264      dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 64)           819264      dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 64)           819264      dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            130         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            195         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            325         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            260         dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            195         dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            195         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            195         dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            260         dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 16,639,995\n",
            "Trainable params: 16,632,251\n",
            "Non-trainable params: 7,744\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_HwQ1MH52C0",
        "colab_type": "text"
      },
      "source": [
        "Try with decresing kernel size "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXfGmagwsSjk",
        "colab_type": "text"
      },
      "source": [
        "# LR FINDER AND CYCLIC LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3eRTqbDSs9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.callbacks import *\n",
        "def find_lr(model, start_lr, end_lr):\n",
        "    finder = LRFinder(start_lr, end_lr, len(train_gen))\n",
        "    weights = model.get_weights()    \n",
        "    try:\n",
        "        history = model.fit_generator(\n",
        "            generator=train_gen,\n",
        "            validation_data=valid_gen,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=[finder],\n",
        "        )   \n",
        "    finally:    \n",
        "        model.set_weights(weights)    \n",
        "    return finder\n",
        "    \n",
        "class LRFinder(Callback):    \n",
        "    def __init__(self, start_lr=1e-5, end_lr=10, step_size=None, beta=.98):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.start_lr = start_lr\n",
        "        self.end_lr = end_lr\n",
        "        self.step_size = step_size\n",
        "        self.beta = beta\n",
        "        self.lr_mult = (end_lr/start_lr)**(1/step_size)\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.best_loss = 1e9\n",
        "        self.avg_loss = 0\n",
        "        self.losses, self.smoothed_losses, self.lrs, self.iterations = [], [], [], []\n",
        "        self.iteration = 0\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.start_lr)\n",
        "        \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        loss = logs.get('loss')\n",
        "        self.iteration += 1\n",
        "        \n",
        "        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * loss\n",
        "        smoothed_loss = self.avg_loss / (1 - self.beta**self.iteration)\n",
        "        \n",
        "        # Check if the loss is not exploding\n",
        "        if self.iteration>1 and smoothed_loss > self.best_loss * 4:\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if smoothed_loss < self.best_loss or self.iteration==1:\n",
        "            self.best_loss = smoothed_loss\n",
        "        \n",
        "        lr = self.start_lr * (self.lr_mult**self.iteration)\n",
        "        \n",
        "        self.losses.append(loss)\n",
        "        self.smoothed_losses.append(smoothed_loss)\n",
        "        self.lrs.append(lr)\n",
        "        self.iterations.append(self.iteration)        \n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, lr)  \n",
        "\n",
        "    def plot(self, lskip=10, rskip=10):\n",
        "        lrs = self.lrs[lskip:-rskip]\n",
        "        losses = self.smoothed_losses[lskip:-rskip]\n",
        "\n",
        "        output_notebook()\n",
        "        p = figure(title='Learning Rate Finder', x_axis_label='LR', y_axis_label='Loss')\n",
        "        p.line(lrs, losses)\n",
        "        show(p)\n",
        "        \n",
        "        best_idxs = np.argpartition(losses, 15)[:15]\n",
        "        best_lrs = np.take(lrs, best_idxs)\n",
        "        print(f\"Best LRs: {best_lrs}\")    \n",
        "#Cyclic Learning Rate\n",
        "class CyclicLR(Callback):\n",
        "  \n",
        "\n",
        "    def __init__(self, base_lr=0.00000005, max_lr=0.009, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HouB0Oq3v_Mq",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer and LR finder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDZhNTWnjAaZ",
        "colab_type": "code",
        "outputId": "6df07a9d-3b0f-4b6e-f77f-49f7e468eac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "source": [
        "opt = RMSprop()\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "      \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "finder = find_lr(model, 1e-4, 1e-2)\n",
        "finder.plot()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "90/90 [==============================] - 48s 529ms/step - loss: 8.2206 - gender_output_loss: 0.6879 - image_quality_output_loss: 1.0033 - age_output_loss: 1.4739 - weight_output_loss: 1.0778 - bag_output_loss: 0.9556 - footwear_output_loss: 1.0373 - pose_output_loss: 0.9667 - emotion_output_loss: 1.0182 - gender_output_acc: 0.5552 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3990 - weight_output_acc: 0.6282 - bag_output_acc: 0.5600 - footwear_output_acc: 0.4726 - pose_output_acc: 0.6138 - emotion_output_acc: 0.7112 - val_loss: 8.4810 - val_gender_output_loss: 0.6880 - val_image_quality_output_loss: 1.0466 - val_age_output_loss: 1.4848 - val_weight_output_loss: 1.0564 - val_bag_output_loss: 0.9429 - val_footwear_output_loss: 1.0807 - val_pose_output_loss: 1.1100 - val_emotion_output_loss: 1.0717 - val_gender_output_acc: 0.5656 - val_image_quality_output_acc: 0.5594 - val_age_output_acc: 0.3688 - val_weight_output_acc: 0.6432 - val_bag_output_acc: 0.5443 - val_footwear_output_acc: 0.4437 - val_pose_output_acc: 0.2620 - val_emotion_output_acc: 0.6865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  var JS_MIME_TYPE = 'application/javascript';\n",
              "  var HTML_MIME_TYPE = 'text/html';\n",
              "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    var script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    var cell = handle.cell;\n",
              "\n",
              "    var id = cell.output_area._bokeh_element_id;\n",
              "    var server_id = cell.output_area._bokeh_server_id;\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null && id in Bokeh.index) {\n",
              "      Bokeh.index[id].model.document.clear();\n",
              "      delete Bokeh.index[id];\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            var id = msg.content.text.trim();\n",
              "            if (id in Bokeh.index) {\n",
              "              Bokeh.index[id].model.document.clear();\n",
              "              delete Bokeh.index[id];\n",
              "            }\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    var output_area = handle.output_area;\n",
              "    var output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      var bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      var script_attrs = bk_div.children[0].attributes;\n",
              "      for (var i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      var toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  \n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    var el = document.getElementById(null);\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "\n",
              "    function on_error() {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    for (var i = 0; i < css_urls.length; i++) {\n",
              "      var url = css_urls[i];\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }\n",
              "\n",
              "    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  \n",
              "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
              "  var css_urls = [];\n",
              "  \n",
              "\n",
              "  var inline_js = [\n",
              "    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "    function(Bokeh) {\n",
              "    \n",
              "    \n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    \n",
              "    if (root.Bokeh !== undefined || force === true) {\n",
              "      \n",
              "    for (var i = 0; i < inline_js.length; i++) {\n",
              "      inline_js[i].call(root, root.Bokeh);\n",
              "    }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      var cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(css_urls, js_urls, function() {\n",
              "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div class=\"bk-root\" id=\"0c317ab9-8d75-470e-b32a-62d47212d07b\" data-root-id=\"1199\"></div>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "    \n",
              "  var docs_json = {\"21509b1d-28a5-468f-b005-dd53e8460669\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1210\",\"type\":\"LinearAxis\"}],\"center\":[{\"id\":\"1214\",\"type\":\"Grid\"},{\"id\":\"1219\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1215\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"1236\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1200\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1226\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1202\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1206\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1204\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1208\",\"type\":\"LinearScale\"}},\"id\":\"1199\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1258\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1220\",\"type\":\"PanTool\"},{\"id\":\"1221\",\"type\":\"WheelZoomTool\"},{\"id\":\"1222\",\"type\":\"BoxZoomTool\"},{\"id\":\"1223\",\"type\":\"SaveTool\"},{\"id\":\"1224\",\"type\":\"ResetTool\"},{\"id\":\"1225\",\"type\":\"HelpTool\"}]},\"id\":\"1226\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1256\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1234\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"data\":{\"x\":[0.00017556762912750032,0.0001847849797422293,0.00019448624389373646,0.0002046968271807524,0.0002154434690031887,0.00022675431258708053,0.0002386589786858585,0.00025118864315095844,0.00026437611857491045,0.000278255940220713,0.00029286445646252424,0.000308239923974515,0.0003244226079171637,0.00034145488738336095,0.0003593813663804636,0.0003782489906389394,0.0003981071705534983,0.000419007910578668,0.00044100594541767493,0.0004641588833612792,0.0004885273571519403,0.0005141751827683941,0.0005411695265464654,0.0005695810810737704,0.000599484250318943,0.0006309573444801954,0.0006640827850634865,0.000698947320727351,0.000735642254459644,0.00077426368268113,0.0008149127469020775,0.0008576958985908976,0.0009027251779484613,0.0009501185073181477,0.0010000000000000044,0.0010525002852777375,0.0011077568505097142,0.001165914401179837,0.0012271252398511957,0.00129154966501489,0.0013593563908785322,0.0014307229891937646,0.001505836354279848,0.0015848931924611215,0.0016681005372000675,0.0017556762912750107,0.0018478497974223009,0.0019448624389373727,0.0020469682718075323,0.0021544346900318955,0.0022675431258708145,0.002386589786858595,0.002511886431509595,0.0026437611857491154,0.0027825594022071417,0.0029286445646252547,0.003082399239745163,0.003244226079171651,0.0034145488738336234,0.003593813663804651,0.0037824899063894097,0.0039810717055349994,0.004190079105786697,0.004410059454176768,0.004641588833612812,0.004885273571519424,0.005141751827683963,0.0054116952654646765,0.005695810810737729,0.005994842503189456],\"y\":[9.162025860383155,9.135431613654388,9.11299874370892,9.083413875625498,9.05661465135545,9.033236304659319,9.012793810494124,8.98957937682521,8.971199091876391,8.953083317749094,8.932164452026129,8.910526622177418,8.883408220084574,8.862629327962011,8.847201072024752,8.823885806123384,8.800029054842739,8.769780603438168,8.744876660817228,8.718561789828964,8.70215765592601,8.673920374731486,8.647226454537169,8.621946133518378,8.595033892862212,8.570106993610334,8.547029990346822,8.527337899130336,8.509837118375282,8.482385869077058,8.462905213581893,8.449734028069583,8.427802725216445,8.409395028361997,8.384796419951964,8.362645557262939,8.343959767531503,8.329005775471666,8.31848234951125,8.298120410481607,8.281326063921512,8.266295874379288,8.25311823722342,8.247282621933683,8.234230809501387,8.219401330900597,8.215337566726802,8.201020461506692,8.194279821917585,8.186981736083009,8.179048807481571,8.179380555724952,8.172367374266358,8.167709238588401,8.156419264894442,8.146121382705147,8.133935245939911,8.12755585362425,8.120720616037216,8.116206001059876,8.110465955291739,8.099415526054049,8.095379623074475,8.088093447505145,8.078959547439704,8.077246938984276,8.07742873648976,8.08076255742095,8.080690701347777,8.082658733549977]},\"selected\":{\"id\":\"1260\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1259\",\"type\":\"UnionRenderers\"}},\"id\":\"1233\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1225\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1260\",\"type\":\"Selection\"},{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1216\",\"type\":\"BasicTicker\"}},\"id\":\"1219\",\"type\":\"Grid\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1235\",\"type\":\"Line\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1261\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"data_source\":{\"id\":\"1233\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1234\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1235\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"1237\",\"type\":\"CDSView\"}},\"id\":\"1236\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"1233\",\"type\":\"ColumnDataSource\"}},\"id\":\"1237\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1224\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1216\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1223\",\"type\":\"SaveTool\"},{\"attributes\":{\"axis_label\":\"Loss\",\"formatter\":{\"id\":\"1256\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1216\",\"type\":\"BasicTicker\"}},\"id\":\"1215\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null},\"id\":\"1202\",\"type\":\"DataRange1d\"},{\"attributes\":{\"overlay\":{\"id\":\"1261\",\"type\":\"BoxAnnotation\"}},\"id\":\"1222\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"ticker\":{\"id\":\"1211\",\"type\":\"BasicTicker\"}},\"id\":\"1214\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1259\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null},\"id\":\"1204\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1206\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1221\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1211\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1208\",\"type\":\"LinearScale\"},{\"attributes\":{\"text\":\"Learning Rate Finder\"},\"id\":\"1200\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1220\",\"type\":\"PanTool\"},{\"attributes\":{\"axis_label\":\"LR\",\"formatter\":{\"id\":\"1258\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1211\",\"type\":\"BasicTicker\"}},\"id\":\"1210\",\"type\":\"LinearAxis\"}],\"root_ids\":[\"1199\"]},\"title\":\"Bokeh Application\",\"version\":\"1.4.0\"}};\n",
              "  var render_items = [{\"docid\":\"21509b1d-28a5-468f-b005-dd53e8460669\",\"roots\":{\"1199\":\"0c317ab9-8d75-470e-b32a-62d47212d07b\"}}];\n",
              "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else {\n",
              "        attempts++;\n",
              "        if (attempts > 100) {\n",
              "          clearInterval(timer);\n",
              "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
              "        }\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "tags": [],
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "1199"
            }
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best LRs: [0.00441006 0.00599484 0.00569581 0.0054117  0.00514175 0.00488527\n",
            " 0.00464159 0.00419008 0.00398107 0.00378249 0.00359381 0.00341455\n",
            " 0.00324423 0.00292864 0.0030824 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTIvtWBawIMH",
        "colab_type": "text"
      },
      "source": [
        "AUGMENTATION AND FIT GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qeXeAS4U0FM",
        "colab_type": "code",
        "outputId": "d74f537c-3ed2-4633-e7b2-5a98c9bc2607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\t# add image augmentation\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df, \n",
        "    augmentation=ImageDataGenerator(\n",
        "        zca_whitening=True,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=False,\n",
        "    )\n",
        ")\n",
        "clr_triangular = CyclicLR(1.8e-4,1e-3,mode='triangular2')\n",
        "history=model.fit_generator(  \n",
        "    steps_per_epoch=20000 // 32,  validation_steps=2000 // 64,\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=8, \n",
        "    epochs=25,\n",
        "    verbose=1,callbacks=[checkpoint,clr_triangular]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:336: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  3/625 [..............................] - ETA: 15:22 - loss: 9.4109 - gender_output_loss: 0.6951 - image_quality_output_loss: 1.0969 - age_output_loss: 1.6043 - weight_output_loss: 1.3667 - bag_output_loss: 1.0913 - footwear_output_loss: 1.0925 - pose_output_loss: 1.0931 - emotion_output_loss: 1.3710 - gender_output_acc: 0.4688 - image_quality_output_acc: 0.4688 - age_output_acc: 0.3438 - weight_output_acc: 0.4688 - bag_output_acc: 0.5208 - footwear_output_acc: 0.4375 - pose_output_acc: 0.4271 - emotion_output_acc: 0.5208"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.263415). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 103s 164ms/step - loss: 7.7580 - gender_output_loss: 0.6758 - image_quality_output_loss: 0.9830 - age_output_loss: 1.4287 - weight_output_loss: 0.9953 - bag_output_loss: 0.9132 - footwear_output_loss: 0.9235 - pose_output_loss: 0.9262 - emotion_output_loss: 0.9122 - gender_output_acc: 0.5764 - image_quality_output_acc: 0.5522 - age_output_acc: 0.4046 - weight_output_acc: 0.6351 - bag_output_acc: 0.5662 - footwear_output_acc: 0.5688 - pose_output_acc: 0.6159 - emotion_output_acc: 0.7179 - val_loss: 7.7799 - val_gender_output_loss: 0.6338 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9375 - val_footwear_output_loss: 0.9363 - val_pose_output_loss: 0.9131 - val_emotion_output_loss: 0.9672 - val_gender_output_acc: 0.6300 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3662 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5144 - val_footwear_output_acc: 0.5935 - val_pose_output_acc: 0.6157 - val_emotion_output_acc: 0.6842\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.77993, saving model to /content/drive/My Drive/Colab Notebooks/check.ckpt\n",
            "Epoch 2/25\n",
            "625/625 [==============================] - 98s 158ms/step - loss: 7.3313 - gender_output_loss: 0.5712 - image_quality_output_loss: 0.9739 - age_output_loss: 1.4010 - weight_output_loss: 0.9706 - bag_output_loss: 0.8764 - footwear_output_loss: 0.8138 - pose_output_loss: 0.8343 - emotion_output_loss: 0.8903 - gender_output_acc: 0.6928 - image_quality_output_acc: 0.5545 - age_output_acc: 0.4011 - weight_output_acc: 0.6341 - bag_output_acc: 0.5904 - footwear_output_acc: 0.6422 - pose_output_acc: 0.6398 - emotion_output_acc: 0.7137 - val_loss: 7.4579 - val_gender_output_loss: 0.5478 - val_image_quality_output_loss: 0.9786 - val_age_output_loss: 1.4358 - val_weight_output_loss: 1.0110 - val_bag_output_loss: 0.9004 - val_footwear_output_loss: 0.9356 - val_pose_output_loss: 0.7038 - val_emotion_output_loss: 0.9451 - val_gender_output_acc: 0.7308 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6414 - val_bag_output_acc: 0.5796 - val_footwear_output_acc: 0.5718 - val_pose_output_acc: 0.6966 - val_emotion_output_acc: 0.6928\n",
            "\n",
            "Epoch 00002: val_loss improved from 7.77993 to 7.45788, saving model to /content/drive/My Drive/Colab Notebooks/check.ckpt\n",
            "Epoch 3/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 6.9032 - gender_output_loss: 0.4741 - image_quality_output_loss: 0.9674 - age_output_loss: 1.3757 - weight_output_loss: 0.9588 - bag_output_loss: 0.8553 - footwear_output_loss: 0.7712 - pose_output_loss: 0.6327 - emotion_output_loss: 0.8679 - gender_output_acc: 0.7697 - image_quality_output_acc: 0.5561 - age_output_acc: 0.4098 - weight_output_acc: 0.6331 - bag_output_acc: 0.6077 - footwear_output_acc: 0.6657 - pose_output_acc: 0.7306 - emotion_output_acc: 0.7166 - val_loss: 6.9308 - val_gender_output_loss: 0.4322 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.3990 - val_weight_output_loss: 0.9476 - val_bag_output_loss: 0.8744 - val_footwear_output_loss: 0.7910 - val_pose_output_loss: 0.5906 - val_emotion_output_loss: 0.9235 - val_gender_output_acc: 0.8039 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3921 - val_weight_output_acc: 0.6489 - val_bag_output_acc: 0.5970 - val_footwear_output_acc: 0.6575 - val_pose_output_acc: 0.7548 - val_emotion_output_acc: 0.6883\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.45788 to 6.93082, saving model to /content/drive/My Drive/Colab Notebooks/check.ckpt\n",
            "Epoch 4/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 6.6244 - gender_output_loss: 0.3999 - image_quality_output_loss: 0.9502 - age_output_loss: 1.3640 - weight_output_loss: 0.9448 - bag_output_loss: 0.8270 - footwear_output_loss: 0.7459 - pose_output_loss: 0.5343 - emotion_output_loss: 0.8583 - gender_output_acc: 0.8169 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4138 - weight_output_acc: 0.6359 - bag_output_acc: 0.6322 - footwear_output_acc: 0.6743 - pose_output_acc: 0.7821 - emotion_output_acc: 0.7152 - val_loss: 6.9489 - val_gender_output_loss: 0.4474 - val_image_quality_output_loss: 0.9722 - val_age_output_loss: 1.4027 - val_weight_output_loss: 0.9417 - val_bag_output_loss: 0.8782 - val_footwear_output_loss: 0.7687 - val_pose_output_loss: 0.6111 - val_emotion_output_loss: 0.9269 - val_gender_output_acc: 0.7916 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3821 - val_weight_output_acc: 0.6444 - val_bag_output_acc: 0.5900 - val_footwear_output_acc: 0.6605 - val_pose_output_acc: 0.7452 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 6.93082\n",
            "Epoch 5/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 6.2093 - gender_output_loss: 0.3029 - image_quality_output_loss: 0.9010 - age_output_loss: 1.3378 - weight_output_loss: 0.9173 - bag_output_loss: 0.7880 - footwear_output_loss: 0.7013 - pose_output_loss: 0.4226 - emotion_output_loss: 0.8384 - gender_output_acc: 0.8728 - image_quality_output_acc: 0.5763 - age_output_acc: 0.4188 - weight_output_acc: 0.6419 - bag_output_acc: 0.6591 - footwear_output_acc: 0.7006 - pose_output_acc: 0.8338 - emotion_output_acc: 0.7183 - val_loss: 6.6912 - val_gender_output_loss: 0.3922 - val_image_quality_output_loss: 0.9153 - val_age_output_loss: 1.3922 - val_weight_output_loss: 0.9340 - val_bag_output_loss: 0.8471 - val_footwear_output_loss: 0.7787 - val_pose_output_loss: 0.5302 - val_emotion_output_loss: 0.9015 - val_gender_output_acc: 0.8395 - val_image_quality_output_acc: 0.5658 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6517 - val_bag_output_acc: 0.6179 - val_footwear_output_acc: 0.6631 - val_pose_output_acc: 0.7913 - val_emotion_output_acc: 0.6888\n",
            "\n",
            "Epoch 00005: val_loss improved from 6.93082 to 6.69120, saving model to /content/drive/My Drive/Colab Notebooks/check.ckpt\n",
            "Epoch 6/25\n",
            "625/625 [==============================] - 100s 159ms/step - loss: 5.7603 - gender_output_loss: 0.2009 - image_quality_output_loss: 0.8665 - age_output_loss: 1.2937 - weight_output_loss: 0.8749 - bag_output_loss: 0.7294 - footwear_output_loss: 0.6524 - pose_output_loss: 0.3199 - emotion_output_loss: 0.8226 - gender_output_acc: 0.9237 - image_quality_output_acc: 0.5920 - age_output_acc: 0.4368 - weight_output_acc: 0.6524 - bag_output_acc: 0.6977 - footwear_output_acc: 0.7227 - pose_output_acc: 0.8813 - emotion_output_acc: 0.7140 - val_loss: 6.8037 - val_gender_output_loss: 0.4026 - val_image_quality_output_loss: 0.9208 - val_age_output_loss: 1.4019 - val_weight_output_loss: 0.9489 - val_bag_output_loss: 0.8484 - val_footwear_output_loss: 0.8252 - val_pose_output_loss: 0.5540 - val_emotion_output_loss: 0.9020 - val_gender_output_acc: 0.8412 - val_image_quality_output_acc: 0.5703 - val_age_output_acc: 0.3846 - val_weight_output_acc: 0.6434 - val_bag_output_acc: 0.6285 - val_footwear_output_acc: 0.6527 - val_pose_output_acc: 0.7931 - val_emotion_output_acc: 0.6903\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 6.69120\n",
            "Epoch 7/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 5.2899 - gender_output_loss: 0.1229 - image_quality_output_loss: 0.8271 - age_output_loss: 1.2363 - weight_output_loss: 0.8223 - bag_output_loss: 0.6809 - footwear_output_loss: 0.5859 - pose_output_loss: 0.2135 - emotion_output_loss: 0.8010 - gender_output_acc: 0.9583 - image_quality_output_acc: 0.6138 - age_output_acc: 0.4682 - weight_output_acc: 0.6710 - bag_output_acc: 0.7208 - footwear_output_acc: 0.7516 - pose_output_acc: 0.9289 - emotion_output_acc: 0.7165 - val_loss: 7.3039 - val_gender_output_loss: 0.4853 - val_image_quality_output_loss: 0.9316 - val_age_output_loss: 1.4501 - val_weight_output_loss: 0.9696 - val_bag_output_loss: 0.9465 - val_footwear_output_loss: 0.9147 - val_pose_output_loss: 0.6829 - val_emotion_output_loss: 0.9234 - val_gender_output_acc: 0.8344 - val_image_quality_output_acc: 0.5592 - val_age_output_acc: 0.3851 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.6091 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.7770 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 6.69120\n",
            "Epoch 8/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 5.0266 - gender_output_loss: 0.0950 - image_quality_output_loss: 0.8057 - age_output_loss: 1.1938 - weight_output_loss: 0.7800 - bag_output_loss: 0.6402 - footwear_output_loss: 0.5459 - pose_output_loss: 0.1786 - emotion_output_loss: 0.7872 - gender_output_acc: 0.9669 - image_quality_output_acc: 0.6257 - age_output_acc: 0.4822 - weight_output_acc: 0.6895 - bag_output_acc: 0.7426 - footwear_output_acc: 0.7726 - pose_output_acc: 0.9397 - emotion_output_acc: 0.7152 - val_loss: 8.0160 - val_gender_output_loss: 0.6717 - val_image_quality_output_loss: 1.2899 - val_age_output_loss: 1.5006 - val_weight_output_loss: 1.0102 - val_bag_output_loss: 0.9449 - val_footwear_output_loss: 0.8911 - val_pose_output_loss: 0.7163 - val_emotion_output_loss: 0.9913 - val_gender_output_acc: 0.8163 - val_image_quality_output_acc: 0.4501 - val_age_output_acc: 0.3737 - val_weight_output_acc: 0.6053 - val_bag_output_acc: 0.6179 - val_footwear_output_acc: 0.6477 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6822\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.69120\n",
            "Epoch 9/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 4.8231 - gender_output_loss: 0.0934 - image_quality_output_loss: 0.7883 - age_output_loss: 1.1520 - weight_output_loss: 0.7375 - bag_output_loss: 0.6060 - footwear_output_loss: 0.5185 - pose_output_loss: 0.1585 - emotion_output_loss: 0.7688 - gender_output_acc: 0.9678 - image_quality_output_acc: 0.6365 - age_output_acc: 0.5021 - weight_output_acc: 0.7057 - bag_output_acc: 0.7582 - footwear_output_acc: 0.7812 - pose_output_acc: 0.9450 - emotion_output_acc: 0.7208 - val_loss: 8.1122 - val_gender_output_loss: 0.5815 - val_image_quality_output_loss: 1.0838 - val_age_output_loss: 1.5182 - val_weight_output_loss: 1.0443 - val_bag_output_loss: 1.0194 - val_footwear_output_loss: 0.9808 - val_pose_output_loss: 0.9298 - val_emotion_output_loss: 0.9544 - val_gender_output_acc: 0.8107 - val_image_quality_output_acc: 0.5353 - val_age_output_acc: 0.3526 - val_weight_output_acc: 0.5948 - val_bag_output_acc: 0.6086 - val_footwear_output_acc: 0.6142 - val_pose_output_acc: 0.7215 - val_emotion_output_acc: 0.6817\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 6.69120\n",
            "Epoch 10/25\n",
            "625/625 [==============================] - 98s 157ms/step - loss: 4.6540 - gender_output_loss: 0.0944 - image_quality_output_loss: 0.7602 - age_output_loss: 1.1083 - weight_output_loss: 0.7022 - bag_output_loss: 0.5795 - footwear_output_loss: 0.4938 - pose_output_loss: 0.1540 - emotion_output_loss: 0.7617 - gender_output_acc: 0.9665 - image_quality_output_acc: 0.6549 - age_output_acc: 0.5252 - weight_output_acc: 0.7210 - bag_output_acc: 0.7706 - footwear_output_acc: 0.7921 - pose_output_acc: 0.9462 - emotion_output_acc: 0.7189 - val_loss: 8.7605 - val_gender_output_loss: 0.8000 - val_image_quality_output_loss: 1.0658 - val_age_output_loss: 1.5822 - val_weight_output_loss: 1.1360 - val_bag_output_loss: 1.2130 - val_footwear_output_loss: 1.0783 - val_pose_output_loss: 0.9288 - val_emotion_output_loss: 0.9564 - val_gender_output_acc: 0.8158 - val_image_quality_output_acc: 0.5368 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.6157 - val_bag_output_acc: 0.6079 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.7540 - val_emotion_output_acc: 0.6893\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.69120\n",
            "Epoch 11/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 4.1395 - gender_output_loss: 0.0715 - image_quality_output_loss: 0.6923 - age_output_loss: 1.0199 - weight_output_loss: 0.6197 - bag_output_loss: 0.4805 - footwear_output_loss: 0.4319 - pose_output_loss: 0.1016 - emotion_output_loss: 0.7220 - gender_output_acc: 0.9745 - image_quality_output_acc: 0.6879 - age_output_acc: 0.5618 - weight_output_acc: 0.7534 - bag_output_acc: 0.8157 - footwear_output_acc: 0.8197 - pose_output_acc: 0.9656 - emotion_output_acc: 0.7333 - val_loss: 8.9796 - val_gender_output_loss: 0.6790 - val_image_quality_output_loss: 1.0839 - val_age_output_loss: 1.6504 - val_weight_output_loss: 1.2239 - val_bag_output_loss: 1.1850 - val_footwear_output_loss: 1.1350 - val_pose_output_loss: 1.0464 - val_emotion_output_loss: 0.9761 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5391 - val_age_output_acc: 0.3730 - val_weight_output_acc: 0.6202 - val_bag_output_acc: 0.5958 - val_footwear_output_acc: 0.6096 - val_pose_output_acc: 0.7621 - val_emotion_output_acc: 0.6678\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 6.69120\n",
            "Epoch 12/25\n",
            "625/625 [==============================] - 98s 157ms/step - loss: 3.3907 - gender_output_loss: 0.0381 - image_quality_output_loss: 0.5756 - age_output_loss: 0.8710 - weight_output_loss: 0.4975 - bag_output_loss: 0.3483 - footwear_output_loss: 0.3388 - pose_output_loss: 0.0585 - emotion_output_loss: 0.6627 - gender_output_acc: 0.9862 - image_quality_output_acc: 0.7525 - age_output_acc: 0.6392 - weight_output_acc: 0.8070 - bag_output_acc: 0.8718 - footwear_output_acc: 0.8632 - pose_output_acc: 0.9806 - emotion_output_acc: 0.7505 - val_loss: 10.7255 - val_gender_output_loss: 0.8998 - val_image_quality_output_loss: 1.7611 - val_age_output_loss: 1.8374 - val_weight_output_loss: 1.3485 - val_bag_output_loss: 1.4525 - val_footwear_output_loss: 1.2542 - val_pose_output_loss: 1.1468 - val_emotion_output_loss: 1.0254 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.4577 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6064 - val_footwear_output_acc: 0.6162 - val_pose_output_acc: 0.7598 - val_emotion_output_acc: 0.6676\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.69120\n",
            "Epoch 13/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 2.6505 - gender_output_loss: 0.0203 - image_quality_output_loss: 0.4325 - age_output_loss: 0.7162 - weight_output_loss: 0.3822 - bag_output_loss: 0.2237 - footwear_output_loss: 0.2488 - pose_output_loss: 0.0356 - emotion_output_loss: 0.5912 - gender_output_acc: 0.9934 - image_quality_output_acc: 0.8249 - age_output_acc: 0.7145 - weight_output_acc: 0.8599 - bag_output_acc: 0.9213 - footwear_output_acc: 0.9062 - pose_output_acc: 0.9890 - emotion_output_acc: 0.7785 - val_loss: 11.3296 - val_gender_output_loss: 0.9859 - val_image_quality_output_loss: 1.4216 - val_age_output_loss: 1.9830 - val_weight_output_loss: 1.4906 - val_bag_output_loss: 1.6716 - val_footwear_output_loss: 1.4420 - val_pose_output_loss: 1.2871 - val_emotion_output_loss: 1.0479 - val_gender_output_acc: 0.8173 - val_image_quality_output_acc: 0.5151 - val_age_output_acc: 0.3654 - val_weight_output_acc: 0.5811 - val_bag_output_acc: 0.6096 - val_footwear_output_acc: 0.6058 - val_pose_output_acc: 0.7664 - val_emotion_output_acc: 0.6293\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 6.69120\n",
            "Epoch 14/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 2.2283 - gender_output_loss: 0.0190 - image_quality_output_loss: 0.3371 - age_output_loss: 0.6228 - weight_output_loss: 0.3171 - bag_output_loss: 0.1632 - footwear_output_loss: 0.2026 - pose_output_loss: 0.0278 - emotion_output_loss: 0.5388 - gender_output_acc: 0.9936 - image_quality_output_acc: 0.8672 - age_output_acc: 0.7593 - weight_output_acc: 0.8822 - bag_output_acc: 0.9429 - footwear_output_acc: 0.9259 - pose_output_acc: 0.9905 - emotion_output_acc: 0.7997 - val_loss: 13.6926 - val_gender_output_loss: 1.1360 - val_image_quality_output_loss: 2.5783 - val_age_output_loss: 2.2110 - val_weight_output_loss: 1.6075 - val_bag_output_loss: 1.9061 - val_footwear_output_loss: 1.7157 - val_pose_output_loss: 1.3607 - val_emotion_output_loss: 1.1775 - val_gender_output_acc: 0.8211 - val_image_quality_output_acc: 0.4297 - val_age_output_acc: 0.3652 - val_weight_output_acc: 0.6038 - val_bag_output_acc: 0.5930 - val_footwear_output_acc: 0.6190 - val_pose_output_acc: 0.7593 - val_emotion_output_acc: 0.6706\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 6.69120\n",
            "Epoch 15/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 2.1041 - gender_output_loss: 0.0242 - image_quality_output_loss: 0.2996 - age_output_loss: 0.5860 - weight_output_loss: 0.3052 - bag_output_loss: 0.1633 - footwear_output_loss: 0.1921 - pose_output_loss: 0.0385 - emotion_output_loss: 0.4951 - gender_output_acc: 0.9918 - image_quality_output_acc: 0.8818 - age_output_acc: 0.7721 - weight_output_acc: 0.8857 - bag_output_acc: 0.9422 - footwear_output_acc: 0.9289 - pose_output_acc: 0.9867 - emotion_output_acc: 0.8179 - val_loss: 13.6943 - val_gender_output_loss: 1.1097 - val_image_quality_output_loss: 2.0363 - val_age_output_loss: 2.2967 - val_weight_output_loss: 1.7166 - val_bag_output_loss: 2.0632 - val_footwear_output_loss: 1.8746 - val_pose_output_loss: 1.4639 - val_emotion_output_loss: 1.1334 - val_gender_output_acc: 0.8107 - val_image_quality_output_acc: 0.4685 - val_age_output_acc: 0.3385 - val_weight_output_acc: 0.5922 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6094 - val_pose_output_acc: 0.7457 - val_emotion_output_acc: 0.6303\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 6.69120\n",
            "Epoch 16/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 2.0169 - gender_output_loss: 0.0304 - image_quality_output_loss: 0.2783 - age_output_loss: 0.5520 - weight_output_loss: 0.2971 - bag_output_loss: 0.1603 - footwear_output_loss: 0.1844 - pose_output_loss: 0.0477 - emotion_output_loss: 0.4668 - gender_output_acc: 0.9892 - image_quality_output_acc: 0.8891 - age_output_acc: 0.7841 - weight_output_acc: 0.8871 - bag_output_acc: 0.9412 - footwear_output_acc: 0.9306 - pose_output_acc: 0.9832 - emotion_output_acc: 0.8300 - val_loss: 14.7992 - val_gender_output_loss: 1.3877 - val_image_quality_output_loss: 1.9691 - val_age_output_loss: 2.5444 - val_weight_output_loss: 1.7623 - val_bag_output_loss: 2.3912 - val_footwear_output_loss: 1.9093 - val_pose_output_loss: 1.5762 - val_emotion_output_loss: 1.2589 - val_gender_output_acc: 0.7818 - val_image_quality_output_acc: 0.4975 - val_age_output_acc: 0.3624 - val_weight_output_acc: 0.5907 - val_bag_output_acc: 0.5388 - val_footwear_output_acc: 0.5922 - val_pose_output_acc: 0.7639 - val_emotion_output_acc: 0.6351\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 6.69120\n",
            "Epoch 17/25\n",
            "625/625 [==============================] - 101s 161ms/step - loss: 1.8824 - gender_output_loss: 0.0302 - image_quality_output_loss: 0.2596 - age_output_loss: 0.5119 - weight_output_loss: 0.2788 - bag_output_loss: 0.1492 - footwear_output_loss: 0.1752 - pose_output_loss: 0.0463 - emotion_output_loss: 0.4312 - gender_output_acc: 0.9888 - image_quality_output_acc: 0.8962 - age_output_acc: 0.8025 - weight_output_acc: 0.8919 - bag_output_acc: 0.9456 - footwear_output_acc: 0.9344 - pose_output_acc: 0.9836 - emotion_output_acc: 0.8465 - val_loss: 15.5523 - val_gender_output_loss: 1.1630 - val_image_quality_output_loss: 2.8635 - val_age_output_loss: 2.5598 - val_weight_output_loss: 1.9195 - val_bag_output_loss: 2.2629 - val_footwear_output_loss: 1.9941 - val_pose_output_loss: 1.5452 - val_emotion_output_loss: 1.2443 - val_gender_output_acc: 0.8183 - val_image_quality_output_acc: 0.4136 - val_age_output_acc: 0.3511 - val_weight_output_acc: 0.5824 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6119 - val_pose_output_acc: 0.7578 - val_emotion_output_acc: 0.6207\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 6.69120\n",
            "Epoch 18/25\n",
            "625/625 [==============================] - 100s 159ms/step - loss: 1.4393 - gender_output_loss: 0.0216 - image_quality_output_loss: 0.1820 - age_output_loss: 0.4006 - weight_output_loss: 0.2161 - bag_output_loss: 0.1060 - footwear_output_loss: 0.1271 - pose_output_loss: 0.0337 - emotion_output_loss: 0.3521 - gender_output_acc: 0.9924 - image_quality_output_acc: 0.9311 - age_output_acc: 0.8523 - weight_output_acc: 0.9197 - bag_output_acc: 0.9616 - footwear_output_acc: 0.9542 - pose_output_acc: 0.9876 - emotion_output_acc: 0.8787 - val_loss: 15.9238 - val_gender_output_loss: 1.1518 - val_image_quality_output_loss: 2.2332 - val_age_output_loss: 2.8477 - val_weight_output_loss: 1.9816 - val_bag_output_loss: 2.5729 - val_footwear_output_loss: 2.2128 - val_pose_output_loss: 1.5964 - val_emotion_output_loss: 1.3275 - val_gender_output_acc: 0.8150 - val_image_quality_output_acc: 0.5159 - val_age_output_acc: 0.3485 - val_weight_output_acc: 0.5857 - val_bag_output_acc: 0.5864 - val_footwear_output_acc: 0.5950 - val_pose_output_acc: 0.7606 - val_emotion_output_acc: 0.6111\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 6.69120\n",
            "Epoch 19/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 1.0578 - gender_output_loss: 0.0149 - image_quality_output_loss: 0.1200 - age_output_loss: 0.3062 - weight_output_loss: 0.1582 - bag_output_loss: 0.0690 - footwear_output_loss: 0.0868 - pose_output_loss: 0.0244 - emotion_output_loss: 0.2784 - gender_output_acc: 0.9945 - image_quality_output_acc: 0.9560 - age_output_acc: 0.8932 - weight_output_acc: 0.9457 - bag_output_acc: 0.9763 - footwear_output_acc: 0.9693 - pose_output_acc: 0.9911 - emotion_output_acc: 0.9066 - val_loss: 17.5486 - val_gender_output_loss: 1.4006 - val_image_quality_output_loss: 2.6571 - val_age_output_loss: 3.1163 - val_weight_output_loss: 2.1557 - val_bag_output_loss: 2.7964 - val_footwear_output_loss: 2.2593 - val_pose_output_loss: 1.6978 - val_emotion_output_loss: 1.4654 - val_gender_output_acc: 0.8090 - val_image_quality_output_acc: 0.5003 - val_age_output_acc: 0.3352 - val_weight_output_acc: 0.5857 - val_bag_output_acc: 0.6031 - val_footwear_output_acc: 0.6119 - val_pose_output_acc: 0.7485 - val_emotion_output_acc: 0.6031\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 6.69120\n",
            "Epoch 20/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 0.7641 - gender_output_loss: 0.0098 - image_quality_output_loss: 0.0794 - age_output_loss: 0.2268 - weight_output_loss: 0.1138 - bag_output_loss: 0.0468 - footwear_output_loss: 0.0671 - pose_output_loss: 0.0161 - emotion_output_loss: 0.2043 - gender_output_acc: 0.9961 - image_quality_output_acc: 0.9716 - age_output_acc: 0.9263 - weight_output_acc: 0.9641 - bag_output_acc: 0.9845 - footwear_output_acc: 0.9779 - pose_output_acc: 0.9946 - emotion_output_acc: 0.9351 - val_loss: 18.0133 - val_gender_output_loss: 1.2882 - val_image_quality_output_loss: 2.7102 - val_age_output_loss: 3.3871 - val_weight_output_loss: 2.3169 - val_bag_output_loss: 2.7167 - val_footwear_output_loss: 2.3655 - val_pose_output_loss: 1.6454 - val_emotion_output_loss: 1.5833 - val_gender_output_acc: 0.8158 - val_image_quality_output_acc: 0.5030 - val_age_output_acc: 0.3392 - val_weight_output_acc: 0.5496 - val_bag_output_acc: 0.5804 - val_footwear_output_acc: 0.6051 - val_pose_output_acc: 0.7626 - val_emotion_output_acc: 0.6089\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 6.69120\n",
            "Epoch 21/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 0.7196 - gender_output_loss: 0.0106 - image_quality_output_loss: 0.0877 - age_output_loss: 0.2085 - weight_output_loss: 0.1077 - bag_output_loss: 0.0503 - footwear_output_loss: 0.0589 - pose_output_loss: 0.0190 - emotion_output_loss: 0.1769 - gender_output_acc: 0.9960 - image_quality_output_acc: 0.9683 - age_output_acc: 0.9306 - weight_output_acc: 0.9646 - bag_output_acc: 0.9820 - footwear_output_acc: 0.9800 - pose_output_acc: 0.9933 - emotion_output_acc: 0.9436 - val_loss: 19.6600 - val_gender_output_loss: 1.4673 - val_image_quality_output_loss: 2.9569 - val_age_output_loss: 3.5924 - val_weight_output_loss: 2.4079 - val_bag_output_loss: 3.1842 - val_footwear_output_loss: 2.5176 - val_pose_output_loss: 1.8798 - val_emotion_output_loss: 1.6539 - val_gender_output_acc: 0.7999 - val_image_quality_output_acc: 0.5020 - val_age_output_acc: 0.3427 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.5166 - val_footwear_output_acc: 0.5990 - val_pose_output_acc: 0.7533 - val_emotion_output_acc: 0.6323\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 6.69120\n",
            "Epoch 22/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 0.7418 - gender_output_loss: 0.0116 - image_quality_output_loss: 0.1010 - age_output_loss: 0.2065 - weight_output_loss: 0.1095 - bag_output_loss: 0.0542 - footwear_output_loss: 0.0671 - pose_output_loss: 0.0254 - emotion_output_loss: 0.1665 - gender_output_acc: 0.9957 - image_quality_output_acc: 0.9612 - age_output_acc: 0.9297 - weight_output_acc: 0.9619 - bag_output_acc: 0.9809 - footwear_output_acc: 0.9757 - pose_output_acc: 0.9911 - emotion_output_acc: 0.9449 - val_loss: 19.9793 - val_gender_output_loss: 1.5452 - val_image_quality_output_loss: 2.9920 - val_age_output_loss: 3.7288 - val_weight_output_loss: 2.5796 - val_bag_output_loss: 3.0592 - val_footwear_output_loss: 2.5149 - val_pose_output_loss: 1.7414 - val_emotion_output_loss: 1.8181 - val_gender_output_acc: 0.8145 - val_image_quality_output_acc: 0.4922 - val_age_output_acc: 0.3483 - val_weight_output_acc: 0.5653 - val_bag_output_acc: 0.5650 - val_footwear_output_acc: 0.6142 - val_pose_output_acc: 0.7553 - val_emotion_output_acc: 0.6137\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 6.69120\n",
            "Epoch 23/25\n",
            "625/625 [==============================] - 98s 157ms/step - loss: 0.7632 - gender_output_loss: 0.0178 - image_quality_output_loss: 0.1054 - age_output_loss: 0.2071 - weight_output_loss: 0.1153 - bag_output_loss: 0.0630 - footwear_output_loss: 0.0680 - pose_output_loss: 0.0239 - emotion_output_loss: 0.1625 - gender_output_acc: 0.9938 - image_quality_output_acc: 0.9605 - age_output_acc: 0.9257 - weight_output_acc: 0.9581 - bag_output_acc: 0.9772 - footwear_output_acc: 0.9745 - pose_output_acc: 0.9919 - emotion_output_acc: 0.9451 - val_loss: 20.1888 - val_gender_output_loss: 1.4962 - val_image_quality_output_loss: 3.1649 - val_age_output_loss: 3.7504 - val_weight_output_loss: 2.4334 - val_bag_output_loss: 3.0023 - val_footwear_output_loss: 2.7439 - val_pose_output_loss: 1.8064 - val_emotion_output_loss: 1.7912 - val_gender_output_acc: 0.8057 - val_image_quality_output_acc: 0.5043 - val_age_output_acc: 0.3455 - val_weight_output_acc: 0.5902 - val_bag_output_acc: 0.5615 - val_footwear_output_acc: 0.6043 - val_pose_output_acc: 0.7623 - val_emotion_output_acc: 0.6109\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 6.69120\n",
            "Epoch 24/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 0.6469 - gender_output_loss: 0.0147 - image_quality_output_loss: 0.0887 - age_output_loss: 0.1763 - weight_output_loss: 0.1007 - bag_output_loss: 0.0532 - footwear_output_loss: 0.0576 - pose_output_loss: 0.0214 - emotion_output_loss: 0.1342 - gender_output_acc: 0.9942 - image_quality_output_acc: 0.9661 - age_output_acc: 0.9397 - weight_output_acc: 0.9660 - bag_output_acc: 0.9811 - footwear_output_acc: 0.9800 - pose_output_acc: 0.9922 - emotion_output_acc: 0.9550 - val_loss: 21.1013 - val_gender_output_loss: 1.6017 - val_image_quality_output_loss: 3.0859 - val_age_output_loss: 3.8882 - val_weight_output_loss: 2.7952 - val_bag_output_loss: 3.1013 - val_footwear_output_loss: 2.7716 - val_pose_output_loss: 1.8910 - val_emotion_output_loss: 1.9664 - val_gender_output_acc: 0.7865 - val_image_quality_output_acc: 0.5088 - val_age_output_acc: 0.3470 - val_weight_output_acc: 0.5486 - val_bag_output_acc: 0.5378 - val_footwear_output_acc: 0.6187 - val_pose_output_acc: 0.7606 - val_emotion_output_acc: 0.6288\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 6.69120\n",
            "Epoch 25/25\n",
            "625/625 [==============================] - 98s 158ms/step - loss: 0.4879 - gender_output_loss: 0.0094 - image_quality_output_loss: 0.0663 - age_output_loss: 0.1355 - weight_output_loss: 0.0733 - bag_output_loss: 0.0402 - footwear_output_loss: 0.0444 - pose_output_loss: 0.0170 - emotion_output_loss: 0.1018 - gender_output_acc: 0.9967 - image_quality_output_acc: 0.9767 - age_output_acc: 0.9563 - weight_output_acc: 0.9750 - bag_output_acc: 0.9861 - footwear_output_acc: 0.9839 - pose_output_acc: 0.9939 - emotion_output_acc: 0.9688 - val_loss: 21.2005 - val_gender_output_loss: 1.5603 - val_image_quality_output_loss: 3.1793 - val_age_output_loss: 3.9781 - val_weight_output_loss: 2.6439 - val_bag_output_loss: 3.0954 - val_footwear_output_loss: 2.8563 - val_pose_output_loss: 1.7839 - val_emotion_output_loss: 2.1033 - val_gender_output_acc: 0.8042 - val_image_quality_output_acc: 0.5076 - val_age_output_acc: 0.3364 - val_weight_output_acc: 0.5731 - val_bag_output_acc: 0.6071 - val_footwear_output_acc: 0.6084 - val_pose_output_acc: 0.7513 - val_emotion_output_acc: 0.5998\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 6.69120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5oImBfQy8yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_stats(history.history, clr_triangular.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyPupLRHo8oQ",
        "colab_type": "code",
        "outputId": "5f94990d-43a3-43eb-8ba0-ad69d1ce4c35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model=load_model('/content/drive/My Drive/Colab Notebooks/check.ckpt')\n",
        "finder = find_lr(model, 1e-7, 1e-3)\n",
        "finder.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "360/360 [==============================] - 55s 153ms/step - loss: 5.8320 - gender_output_loss: 0.2160 - image_quality_output_loss: 0.8718 - age_output_loss: 1.2995 - weight_output_loss: 0.8839 - bag_output_loss: 0.7421 - footwear_output_loss: 0.6571 - pose_output_loss: 0.3363 - emotion_output_loss: 0.8252 - gender_output_acc: 0.9179 - image_quality_output_acc: 0.5897 - age_output_acc: 0.4367 - weight_output_acc: 0.6478 - bag_output_acc: 0.6845 - footwear_output_acc: 0.7201 - pose_output_acc: 0.8776 - emotion_output_acc: 0.7161 - val_loss: 7.5416 - val_gender_output_loss: 0.7014 - val_image_quality_output_loss: 1.0010 - val_age_output_loss: 1.4594 - val_weight_output_loss: 0.9650 - val_bag_output_loss: 1.0066 - val_footwear_output_loss: 0.8209 - val_pose_output_loss: 0.6358 - val_emotion_output_loss: 0.9515 - val_gender_output_acc: 0.7484 - val_image_quality_output_acc: 0.5391 - val_age_output_acc: 0.3302 - val_weight_output_acc: 0.6427 - val_bag_output_acc: 0.5854 - val_footwear_output_acc: 0.6625 - val_pose_output_acc: 0.7354 - val_emotion_output_acc: 0.6823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  var JS_MIME_TYPE = 'application/javascript';\n",
              "  var HTML_MIME_TYPE = 'text/html';\n",
              "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    var script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    var cell = handle.cell;\n",
              "\n",
              "    var id = cell.output_area._bokeh_element_id;\n",
              "    var server_id = cell.output_area._bokeh_server_id;\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null && id in Bokeh.index) {\n",
              "      Bokeh.index[id].model.document.clear();\n",
              "      delete Bokeh.index[id];\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            var id = msg.content.text.trim();\n",
              "            if (id in Bokeh.index) {\n",
              "              Bokeh.index[id].model.document.clear();\n",
              "              delete Bokeh.index[id];\n",
              "            }\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    var output_area = handle.output_area;\n",
              "    var output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      var bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      var script_attrs = bk_div.children[0].attributes;\n",
              "      for (var i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      var toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  \n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    var el = document.getElementById(null);\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "\n",
              "    function on_error() {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    for (var i = 0; i < css_urls.length; i++) {\n",
              "      var url = css_urls[i];\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }\n",
              "\n",
              "    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  \n",
              "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
              "  var css_urls = [];\n",
              "  \n",
              "\n",
              "  var inline_js = [\n",
              "    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "    function(Bokeh) {\n",
              "    \n",
              "    \n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    \n",
              "    if (root.Bokeh !== undefined || force === true) {\n",
              "      \n",
              "    for (var i = 0; i < inline_js.length; i++) {\n",
              "      inline_js[i].call(root, root.Bokeh);\n",
              "    }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      var cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(css_urls, js_urls, function() {\n",
              "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div class=\"bk-root\" id=\"1f6712fc-9132-4fd6-aada-87987d53b432\" data-root-id=\"1310\"></div>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "    \n",
              "  var docs_json = {\"fa630a53-4bfc-48ed-b199-1dfe25524c9e\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1321\",\"type\":\"LinearAxis\"}],\"center\":[{\"id\":\"1325\",\"type\":\"Grid\"},{\"id\":\"1330\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1326\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"1347\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1311\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1337\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1313\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1317\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1315\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1319\",\"type\":\"LinearScale\"}},\"id\":\"1310\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis_label\":\"LR\",\"formatter\":{\"id\":\"1377\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1322\",\"type\":\"BasicTicker\"}},\"id\":\"1321\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1380\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1346\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1377\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1332\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"text\":\"Learning Rate Finder\"},\"id\":\"1311\",\"type\":\"Title\"},{\"attributes\":{\"overlay\":{\"id\":\"1380\",\"type\":\"BoxAnnotation\"}},\"id\":\"1333\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1336\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1375\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1319\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1379\",\"type\":\"Selection\"},{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1327\",\"type\":\"BasicTicker\"}},\"id\":\"1330\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1331\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1317\",\"type\":\"LinearScale\"},{\"attributes\":{\"ticker\":{\"id\":\"1322\",\"type\":\"BasicTicker\"}},\"id\":\"1325\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"1344\",\"type\":\"ColumnDataSource\"}},\"id\":\"1348\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1378\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1327\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"1344\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1345\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1346\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"1348\",\"type\":\"CDSView\"}},\"id\":\"1347\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1322\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1335\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1345\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1334\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1315\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null},\"id\":\"1313\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"data\":{\"x\":[1.3250193550567479e-07,1.3593563908785253e-07,1.3945832491957448e-07,1.4307229891937572e-07,1.467799267622069e-07,1.50583635427984e-07,1.544859147902675e-07,1.5848931924611128e-07,1.6259646938814807e-07,1.668100537200058e-07,1.71132830416178e-07,1.7556762912750006e-07,1.8011735283341322e-07,1.84784979742229e-07,1.8957356524063747e-07,1.9448624389373611e-07,1.9952623149688784e-07,2.0469682718075196e-07,2.1000141557086542e-07,2.1544346900318824e-07,2.2102654979706358e-07,2.2675431258708003e-07,2.3263050671536244e-07,2.386589786858579e-07,2.448436746822225e-07,2.511886431509578e-07,2.5769803745148763e-07,2.6437611857490973e-07,2.712272579332026e-07,2.782559402207122e-07,2.85466766349793e-07,2.9286445646252335e-07,3.00453853020469e-07,3.0823992397451405e-07,3.162277660168376e-07,3.244226079171627e-07,3.328298139454618e-07,3.4145488738335983e-07,3.5030347412653323e-07,3.5938136638046233e-07,3.6869450645195713e-07,3.7824899063893805e-07,3.880510732210178e-07,3.981071705534968e-07,4.084238652674516e-07,4.190079105786664e-07,4.2986623470822724e-07,4.410059454176732e-07,4.5243433466167396e-07,4.641588833612773e-07,4.761872663008531e-07,4.885273571519383e-07,5.011872336272717e-07,5.141751827683919e-07,5.274997063702612e-07,5.41169526546463e-07,5.551935914386202e-07,5.695810810737678e-07,5.843414133735168e-07,5.994842503189401e-07,6.1501950427522e-07,6.309573444801923e-07,6.473082037010389e-07,6.640827850634832e-07,6.812920690579602e-07,6.989473207273474e-07,7.1706009704096e-07,7.356422544596402e-07,7.547059566968892e-07,7.742636826811258e-07,7.943282347242802e-07,8.149127469020729e-07,8.360306936514629e-07,8.576958985908927e-07,8.799225435691056e-07,9.027251779484559e-07,9.261187281287919e-07,9.501185073181421e-07,9.747402255566045e-07,9.999999999999983e-07,1.0259143654700081e-06,1.0525002852777308e-06,1.0797751623277076e-06,1.1077568505097071e-06,1.1364636663857225e-06,1.1659144011798295e-06,1.1961283330787512e-06,1.2271252398511877e-06,1.2589254117941646e-06,1.2915496650148814e-06,1.3250193550567457e-06,1.3593563908785227e-06,1.3945832491957422e-06,1.4307229891937547e-06,1.4677992676220664e-06,1.5058363542798374e-06,1.5448591479026722e-06,1.58489319246111e-06,1.6259646938814779e-06,1.668100537200055e-06,1.711328304161777e-06,1.7556762912749973e-06,1.801173528334129e-06,1.8478497974222868e-06,1.8957356524063715e-06,1.9448624389373577e-06,1.995262314968875e-06,2.046968271807516e-06,2.1000141557086502e-06,2.1544346900318784e-06,2.210265497970632e-06,2.2675431258707964e-06,2.3263050671536204e-06,2.386589786858575e-06,2.448436746822221e-06,2.511886431509574e-06,2.5769803745148717e-06,2.6437611857490928e-06,2.7122725793320213e-06,2.7825594022071175e-06,2.8546676634979246e-06,2.928644564625229e-06,3.0045385302046847e-06,3.082399239745135e-06,3.162277660168371e-06,3.244226079171621e-06,3.328298139454612e-06,3.414548873833592e-06,3.503034741265326e-06,3.5938136638046173e-06,3.6869450645195647e-06,3.7824899063893736e-06,3.880510732210171e-06,3.9810717055349615e-06,4.084238652674509e-06,4.190079105786657e-06,4.298662347082264e-06,4.410059454176724e-06,4.524343346616732e-06,4.641588833612765e-06,4.7618726630085226e-06,4.885273571519374e-06,5.011872336272707e-06,5.14175182768391e-06,5.274997063702602e-06,5.41169526546462e-06,5.551935914386192e-06,5.695810810737669e-06,5.843414133735157e-06,5.994842503189391e-06,6.150195042752189e-06,6.309573444801912e-06,6.473082037010377e-06,6.640827850634819e-06,6.812920690579591e-06,6.989473207273461e-06,7.170600970409587e-06,7.356422544596388e-06,7.547059566968879e-06,7.742636826811243e-06,7.943282347242788e-06,8.149127469020715e-06,8.360306936514614e-06,8.576958985908911e-06,8.79922543569104e-06,9.027251779484543e-06,9.261187281287902e-06,9.501185073181402e-06,9.747402255566028e-06,9.999999999999964e-06,1.025914365470006e-05,1.0525002852777291e-05,1.0797751623277057e-05,1.107756850509705e-05,1.1364636663857205e-05,1.1659144011798273e-05,1.196128333078749e-05,1.2271252398511855e-05,1.2589254117941626e-05,1.291549665014879e-05,1.3250193550567433e-05,1.3593563908785205e-05,1.3945832491957397e-05,1.4307229891937522e-05,1.467799267622064e-05,1.5058363542798345e-05,1.5448591479026695e-05,1.5848931924611073e-05,1.625964693881475e-05,1.668100537200052e-05,1.711328304161774e-05,1.755676291274994e-05,1.8011735283341257e-05,1.8478497974222836e-05,1.895735652406368e-05,1.9448624389373542e-05,1.995262314968871e-05,2.0469682718075124e-05,2.1000141557086466e-05,2.154434690031875e-05,2.210265497970628e-05,2.2675431258707922e-05,2.326305067153616e-05,2.3865897868585707e-05,2.4484367468222166e-05,2.5118864315095693e-05,2.576980374514867e-05,2.6437611857490876e-05,2.7122725793320164e-05,2.7825594022071124e-05,2.8546676634979194e-05,2.9286445646252235e-05,3.0045385302046793e-05,3.08239923974513e-05,3.162277660168365e-05,3.2442260791716156e-05,3.328298139454606e-05,3.4145488738335866e-05,3.50303474126532e-05,3.5938136638046107e-05,3.686945064519559e-05,3.782489906389367e-05,3.8805107322101644e-05,3.981071705534954e-05,4.084238652674502e-05,4.190079105786649e-05,4.298662347082257e-05,4.410059454176716e-05,4.524343346616724e-05,4.641588833612757e-05,4.761872663008514e-05,4.8852735715193655e-05,5.011872336272699e-05,5.141751827683901e-05,5.2749970637025933e-05,5.41169526546461e-05,5.551935914386182e-05,5.6958108107376586e-05,5.843414133735147e-05,5.9948425031893804e-05,6.150195042752178e-05,6.309573444801901e-05,6.473082037010366e-05,6.640827850634809e-05,6.812920690579578e-05,6.98947320727345e-05,7.170600970409576e-05,7.356422544596375e-05,7.547059566968866e-05,7.742636826811231e-05,7.943282347242773e-05,8.1491274690207e-05,8.3603069365146e-05,8.576958985908897e-05,8.799225435691024e-05,9.027251779484528e-05,9.261187281287885e-05,9.501185073181385e-05,9.747402255566011e-05,9.999999999999946e-05,0.00010259143654700043,0.00010525002852777272,0.00010797751623277037,0.0001107756850509703,0.00011364636663857185,0.00011659144011798253,0.00011961283330787471,0.00012271252398511832,0.000125892541179416,0.00012915496650148768,0.0001325019355056741,0.00013593563908785182,0.00013945832491957374,0.00014307229891937496,0.00014677992676220613,0.0001505836354279832,0.00015448591479026667,0.00015848931924611044,0.0001625964693881472,0.00016681005372000494,0.0001711328304161771,0.0001755676291274991,0.00018011735283341226,0.00018478497974222803,0.00018957356524063648,0.00019448624389373508,0.0001995262314968868,0.00020469682718075088,0.0002100014155708643,0.0002154434690031871,0.00022102654979706238,0.0002267543125870788,0.00023263050671536119,0.00023865897868585666,0.0002448436746822212,0.0002511886431509565,0.00025769803745148625,0.00026437611857490833,0.0002712272579332012,0.00027825594022071073,0.0002854667663497914,0.00029286445646252185,0.0003004538530204674,0.00030823992397451244,0.00031622776601683593,0.000324422607917161,0.00033282981394546,0.000341454887383358,0.0003503034741265314,0.00035938136638046046,0.00036869450645195517,0.0003782489906389361,0.0003880510732210157,0.0003981071705534947,0.0004084238652674495,0.0004190079105786642,0.00042986623470822485,0.00044100594541767086,0.0004524343346616716,0.0004641588833612749,0.00047618726630085055,0.0004885273571519357,0.000501187233627269,0.0005141751827683891,0.0005274997063702584,0.0005411695265464601,0.0005551935914386172,0.0005695810810737648,0.0005843414133735136,0.000599484250318937,0.0006150195042752167,0.000630957344480189,0.0006473082037010354,0.0006640827850634797,0.0006812920690579566,0.0006989473207273437,0.0007170600970409562,0.0007356422544596363,0.0007547059566968853,0.0007742636826811217],\"y\":[5.974348858507442,6.009564206047899,6.026794328975094,6.019387470510319,5.968137420211035,5.9339203609443345,5.9429224491937385,5.927885347707902,5.94710006395562,5.971941270738552,6.006002053851017,6.024605762986112,6.009490526288018,6.024553811255111,6.016349837371721,6.012755368068108,5.987384828380503,5.990408469996261,6.021910003903505,6.004330445692882,5.978867569442498,5.9634350358151655,5.9558835452747445,5.964624782460523,5.950802130090151,5.950948569528499,5.955771155025202,5.936080160853948,5.926951740219967,5.928739228005767,5.914766080704782,5.916422344294873,5.916352508779469,5.911232020654116,5.903755049507668,5.882570939184478,5.887576961487188,5.881056246640589,5.886540073012067,5.893669681690572,5.889346205727491,5.873057848559811,5.8668277273813745,5.867024959855843,5.855238906040117,5.851461083495715,5.859027174708723,5.8493660346163985,5.843351013779816,5.843201709824493,5.827532606972815,5.815996800876056,5.827743352798509,5.824380960731008,5.827519764179224,5.835386515999955,5.834999645294885,5.839385596364886,5.835853179155156,5.833860733338214,5.845430240467795,5.854196611583074,5.850035390555456,5.843441343934965,5.837914719586587,5.8371940776000395,5.837574848668161,5.836250185379412,5.8499470134192535,5.852453803140662,5.85302131060447,5.857459495920655,5.857905242441818,5.84995333458932,5.850165014941552,5.843495760037817,5.836843510754237,5.8404241058807065,5.837579130284879,5.8333077977281595,5.833786926867945,5.828253445454961,5.826419550471639,5.814213502320112,5.819696047410461,5.834765713296903,5.827649166786097,5.823883604917417,5.828079499753556,5.834444428929421,5.8383371059998455,5.845370382279764,5.845580232716384,5.867436019079503,5.8787151254931,5.88227503080862,5.879233311432491,5.864557049459473,5.857225300727037,5.862750536043177,5.864887066742181,5.863822109247374,5.8728153521581286,5.8686798413476575,5.862955555282763,5.847896913809154,5.842547791027728,5.8450514456540565,5.836755440710744,5.847821009054695,5.857837711224292,5.867342421296397,5.8585130250255135,5.860816300874507,5.886365526311337,5.880376910523388,5.884764063580332,5.875861262412206,5.884597396118401,5.877546783653123,5.8894357370322235,5.880187824704158,5.878241085432795,5.877167632575859,5.886480525393701,5.88175819958356,5.891536840580961,5.89244551992268,5.893207366179932,5.888309104478258,5.889003843305339,5.883733473796496,5.8925645919062735,5.8846238065921135,5.898397348115409,5.89464560773563,5.878840846008431,5.8822574507634116,5.871767065411061,5.868109969224368,5.872609692786515,5.868248836791085,5.872828661646642,5.859799402518462,5.856509492580876,5.84025581111698,5.848840583641497,5.851913570695311,5.854194516663399,5.860459356465831,5.8549315185521476,5.869908780968092,5.859710056814698,5.870411003013979,5.8586779883063995,5.848664036036994,5.853435945143853,5.84529674109787,5.837630372929716,5.84170789814103,5.846780782826014,5.86500018461935,5.864051029241756,5.860902946073994,5.873973630297765,5.866437329907047,5.868430963485755,5.8648496608073515,5.86102028144007,5.868102542984591,5.856355100674681,5.860739153974894,5.861896155027124,5.864734341554427,5.8607972654185945,5.865841565018783,5.85787442916541,5.8594608209979295,5.859171813657278,5.855279815489321,5.854900006063294,5.854672912008722,5.853882690423206,5.845723896638192,5.83532141472082,5.835919772994116,5.84023526581498,5.842958320115006,5.849674487646374,5.852583065906703,5.839098108635046,5.839443088406543,5.835028700384527,5.833951627540705,5.8276176343029045,5.830012506185785,5.844363820709719,5.839381293226758,5.839052352409266,5.838727000871017,5.847262526017023,5.835335405752885,5.827663177070065,5.8292178022888415,5.817381658027985,5.80808067275572,5.8182732524875815,5.81919335917479,5.8170328061832315,5.824458521869518,5.829405084397654,5.825995457436724,5.824901606360723,5.821088719914586,5.822870268255535,5.823611920442215,5.828748138416932,5.818912230294547,5.808344532093838,5.81226488230369,5.814026594489788,5.820219265465449,5.820489107841491,5.815728261056871,5.7974635281224876,5.79595192277166,5.800620204302003,5.816404477111346,5.823612292912499,5.812279154327032,5.811621283166684,5.807465572812736,5.798018382323088,5.797546104587738,5.801905114649902,5.799523983377563,5.806179715091056,5.807600174193764,5.805170945246904,5.808396361161515,5.810144959714259,5.810160100663616,5.823891609266386,5.824833654827709,5.842291491310291,5.832551318616721,5.824791557325457,5.8295713523080535,5.83055347208515,5.822995168802009,5.823141191254323,5.819151658961641,5.832011617969622,5.831913345936207,5.823978827936545,5.832836880723093,5.824956999606129,5.824434786405048,5.827841632011516,5.81498028072331,5.811626750904714,5.7964902542017125,5.805828707047409,5.798672220890877,5.804389289095877,5.804548664244559,5.79550392683101,5.798471507263362,5.792505507731345,5.804051025057274,5.798176119802309,5.791968905133378,5.784452512448372,5.7808111873759875,5.78750809797727,5.790440125356278,5.793958591620192,5.792381659213711,5.795182206214094,5.802315798831864,5.7972850814649926,5.7815086429394835,5.790851855946891,5.7959030021613485,5.800878030769278,5.80475867511068,5.799329731032755,5.79996775382205,5.798303885581276,5.794520584097055,5.806998213975412,5.8054846522752666,5.812905311040799,5.812720358084817,5.805117236936354,5.796684445477384,5.8021224228010695,5.788377212607406,5.794944866454401,5.793362248813134,5.789673802906838,5.803565597225467,5.8042620254346975,5.797182245753634,5.798745776628469,5.804847230599819,5.79508763149681,5.799476942718217,5.807651604013546,5.8104506654627865,5.801839069845118,5.802464820578106,5.797362588374663,5.785026959932291,5.779619703344477,5.773570871904344,5.7695214208813095,5.769302105798271,5.7656498129268305,5.767607278278888,5.7671508037703685,5.755581444509317,5.758497485190896,5.755682449082386,5.767456664509451,5.759363595852756,5.7497650298015275,5.751436052569622,5.749940500721155,5.7418442750863825,5.731014462559186,5.731495323274899,5.746462924892774,5.738683122063007,5.738497202237997,5.748182951528375,5.749716056530043,5.750413957273512,5.748064669760628,5.748151668037611]},\"selected\":{\"id\":\"1379\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1378\",\"type\":\"UnionRenderers\"}},\"id\":\"1344\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1331\",\"type\":\"PanTool\"},{\"id\":\"1332\",\"type\":\"WheelZoomTool\"},{\"id\":\"1333\",\"type\":\"BoxZoomTool\"},{\"id\":\"1334\",\"type\":\"SaveTool\"},{\"id\":\"1335\",\"type\":\"ResetTool\"},{\"id\":\"1336\",\"type\":\"HelpTool\"}]},\"id\":\"1337\",\"type\":\"Toolbar\"},{\"attributes\":{\"axis_label\":\"Loss\",\"formatter\":{\"id\":\"1375\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1327\",\"type\":\"BasicTicker\"}},\"id\":\"1326\",\"type\":\"LinearAxis\"}],\"root_ids\":[\"1310\"]},\"title\":\"Bokeh Application\",\"version\":\"1.4.0\"}};\n",
              "  var render_items = [{\"docid\":\"fa630a53-4bfc-48ed-b199-1dfe25524c9e\",\"roots\":{\"1310\":\"1f6712fc-9132-4fd6-aada-87987d53b432\"}}];\n",
              "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else {\n",
              "        attempts++;\n",
              "        if (attempts > 100) {\n",
              "          clearInterval(timer);\n",
              "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
              "        }\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "tags": [],
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "1310"
            }
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best LRs: [0.00059948 0.00077426 0.00075471 0.00055519 0.00071706 0.00069895\n",
            " 0.00068129 0.00066408 0.00064731 0.00063096 0.00061502 0.00058434\n",
            " 0.00073564 0.00056958 0.00048853]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4QiEsuCaq5-",
        "colab_type": "code",
        "outputId": "6f96e116-e69f-4781-bbe7-11c2d435664c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\t# add image augmentation\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df, \n",
        "    augmentation=ImageDataGenerator(\n",
        "        zca_whitening=True,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=False,\n",
        "    )\n",
        ")\n",
        "clr_triangular = CyclicLR(1e-6,1e-5,mode='triangular2')\n",
        "history=model.fit_generator(  \n",
        "    steps_per_epoch=20000 // 32,  validation_steps=2000 // 64,\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=8, \n",
        "    epochs=25,\n",
        "    verbose=1,callbacks=[checkpoint,clr_triangular]\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:336: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  2/625 [..............................] - ETA: 19:21 - loss: 5.5312 - gender_output_loss: 0.2068 - image_quality_output_loss: 0.8833 - age_output_loss: 1.2440 - weight_output_loss: 0.7469 - bag_output_loss: 0.8298 - footwear_output_loss: 0.6058 - pose_output_loss: 0.2863 - emotion_output_loss: 0.7283 - gender_output_acc: 0.9219 - image_quality_output_acc: 0.5938 - age_output_acc: 0.4375 - weight_output_acc: 0.7188 - bag_output_acc: 0.6250 - footwear_output_acc: 0.7344 - pose_output_acc: 0.8750 - emotion_output_acc: 0.7656"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (1.276692). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.641567). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 101s 162ms/step - loss: 5.8254 - gender_output_loss: 0.2103 - image_quality_output_loss: 0.8717 - age_output_loss: 1.2990 - weight_output_loss: 0.8827 - bag_output_loss: 0.7449 - footwear_output_loss: 0.6547 - pose_output_loss: 0.3340 - emotion_output_loss: 0.8283 - gender_output_acc: 0.9224 - image_quality_output_acc: 0.5931 - age_output_acc: 0.4371 - weight_output_acc: 0.6465 - bag_output_acc: 0.6847 - footwear_output_acc: 0.7227 - pose_output_acc: 0.8778 - emotion_output_acc: 0.7144 - val_loss: 6.6118 - val_gender_output_loss: 0.3532 - val_image_quality_output_loss: 0.9115 - val_age_output_loss: 1.3859 - val_weight_output_loss: 0.9299 - val_bag_output_loss: 0.8379 - val_footwear_output_loss: 0.7715 - val_pose_output_loss: 0.5169 - val_emotion_output_loss: 0.9049 - val_gender_output_acc: 0.8571 - val_image_quality_output_acc: 0.5731 - val_age_output_acc: 0.3934 - val_weight_output_acc: 0.6575 - val_bag_output_acc: 0.6275 - val_footwear_output_acc: 0.6724 - val_pose_output_acc: 0.7921 - val_emotion_output_acc: 0.6885\n",
            "\n",
            "Epoch 00001: val_loss improved from 6.69120 to 6.61179, saving model to /content/drive/My Drive/Colab Notebooks/check.ckpt\n",
            "Epoch 2/25\n",
            "625/625 [==============================] - 100s 159ms/step - loss: 5.7419 - gender_output_loss: 0.1946 - image_quality_output_loss: 0.8647 - age_output_loss: 1.2931 - weight_output_loss: 0.8790 - bag_output_loss: 0.7306 - footwear_output_loss: 0.6399 - pose_output_loss: 0.3216 - emotion_output_loss: 0.8186 - gender_output_acc: 0.9297 - image_quality_output_acc: 0.5972 - age_output_acc: 0.4350 - weight_output_acc: 0.6511 - bag_output_acc: 0.6932 - footwear_output_acc: 0.7281 - pose_output_acc: 0.8859 - emotion_output_acc: 0.7172 - val_loss: 6.6345 - val_gender_output_loss: 0.3588 - val_image_quality_output_loss: 0.9060 - val_age_output_loss: 1.3889 - val_weight_output_loss: 0.9321 - val_bag_output_loss: 0.8449 - val_footwear_output_loss: 0.7808 - val_pose_output_loss: 0.5169 - val_emotion_output_loss: 0.9062 - val_gender_output_acc: 0.8493 - val_image_quality_output_acc: 0.5736 - val_age_output_acc: 0.3939 - val_weight_output_acc: 0.6494 - val_bag_output_acc: 0.6265 - val_footwear_output_acc: 0.6661 - val_pose_output_acc: 0.7898 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 6.61179\n",
            "Epoch 3/25\n",
            "625/625 [==============================] - 100s 159ms/step - loss: 5.7064 - gender_output_loss: 0.1876 - image_quality_output_loss: 0.8643 - age_output_loss: 1.2881 - weight_output_loss: 0.8693 - bag_output_loss: 0.7287 - footwear_output_loss: 0.6471 - pose_output_loss: 0.3033 - emotion_output_loss: 0.8181 - gender_output_acc: 0.9319 - image_quality_output_acc: 0.5951 - age_output_acc: 0.4416 - weight_output_acc: 0.6526 - bag_output_acc: 0.6955 - footwear_output_acc: 0.7283 - pose_output_acc: 0.8921 - emotion_output_acc: 0.7170 - val_loss: 6.6392 - val_gender_output_loss: 0.3607 - val_image_quality_output_loss: 0.9084 - val_age_output_loss: 1.3938 - val_weight_output_loss: 0.9360 - val_bag_output_loss: 0.8398 - val_footwear_output_loss: 0.7650 - val_pose_output_loss: 0.5166 - val_emotion_output_loss: 0.9190 - val_gender_output_acc: 0.8480 - val_image_quality_output_acc: 0.5771 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6525 - val_bag_output_acc: 0.6285 - val_footwear_output_acc: 0.6711 - val_pose_output_acc: 0.7936 - val_emotion_output_acc: 0.6815\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 6.61179\n",
            "Epoch 4/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 5.6728 - gender_output_loss: 0.1797 - image_quality_output_loss: 0.8564 - age_output_loss: 1.2902 - weight_output_loss: 0.8672 - bag_output_loss: 0.7277 - footwear_output_loss: 0.6374 - pose_output_loss: 0.2957 - emotion_output_loss: 0.8185 - gender_output_acc: 0.9347 - image_quality_output_acc: 0.5961 - age_output_acc: 0.4364 - weight_output_acc: 0.6540 - bag_output_acc: 0.6941 - footwear_output_acc: 0.7326 - pose_output_acc: 0.8962 - emotion_output_acc: 0.7166 - val_loss: 6.6228 - val_gender_output_loss: 0.3595 - val_image_quality_output_loss: 0.9036 - val_age_output_loss: 1.3951 - val_weight_output_loss: 0.9330 - val_bag_output_loss: 0.8455 - val_footwear_output_loss: 0.7639 - val_pose_output_loss: 0.5216 - val_emotion_output_loss: 0.9006 - val_gender_output_acc: 0.8533 - val_image_quality_output_acc: 0.5827 - val_age_output_acc: 0.3926 - val_weight_output_acc: 0.6502 - val_bag_output_acc: 0.6217 - val_footwear_output_acc: 0.6739 - val_pose_output_acc: 0.7949 - val_emotion_output_acc: 0.6938\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 6.61179\n",
            "Epoch 5/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 5.6232 - gender_output_loss: 0.1705 - image_quality_output_loss: 0.8545 - age_output_loss: 1.2798 - weight_output_loss: 0.8633 - bag_output_loss: 0.7196 - footwear_output_loss: 0.6303 - pose_output_loss: 0.2917 - emotion_output_loss: 0.8135 - gender_output_acc: 0.9390 - image_quality_output_acc: 0.6021 - age_output_acc: 0.4402 - weight_output_acc: 0.6580 - bag_output_acc: 0.6987 - footwear_output_acc: 0.7348 - pose_output_acc: 0.8976 - emotion_output_acc: 0.7167 - val_loss: 6.6985 - val_gender_output_loss: 0.3691 - val_image_quality_output_loss: 0.9183 - val_age_output_loss: 1.4001 - val_weight_output_loss: 0.9401 - val_bag_output_loss: 0.8518 - val_footwear_output_loss: 0.7888 - val_pose_output_loss: 0.5257 - val_emotion_output_loss: 0.9046 - val_gender_output_acc: 0.8483 - val_image_quality_output_acc: 0.5703 - val_age_output_acc: 0.3916 - val_weight_output_acc: 0.6505 - val_bag_output_acc: 0.6247 - val_footwear_output_acc: 0.6628 - val_pose_output_acc: 0.7994 - val_emotion_output_acc: 0.6867\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 6.61179\n",
            "Epoch 6/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 5.6320 - gender_output_loss: 0.1705 - image_quality_output_loss: 0.8548 - age_output_loss: 1.2857 - weight_output_loss: 0.8651 - bag_output_loss: 0.7200 - footwear_output_loss: 0.6309 - pose_output_loss: 0.2881 - emotion_output_loss: 0.8167 - gender_output_acc: 0.9388 - image_quality_output_acc: 0.5953 - age_output_acc: 0.4417 - weight_output_acc: 0.6540 - bag_output_acc: 0.7005 - footwear_output_acc: 0.7318 - pose_output_acc: 0.8982 - emotion_output_acc: 0.7157 - val_loss: 6.6489 - val_gender_output_loss: 0.3646 - val_image_quality_output_loss: 0.9056 - val_age_output_loss: 1.3942 - val_weight_output_loss: 0.9402 - val_bag_output_loss: 0.8401 - val_footwear_output_loss: 0.7775 - val_pose_output_loss: 0.5107 - val_emotion_output_loss: 0.9159 - val_gender_output_acc: 0.8538 - val_image_quality_output_acc: 0.5756 - val_age_output_acc: 0.3911 - val_weight_output_acc: 0.6462 - val_bag_output_acc: 0.6280 - val_footwear_output_acc: 0.6641 - val_pose_output_acc: 0.7986 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 6.61179\n",
            "Epoch 7/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 5.5977 - gender_output_loss: 0.1664 - image_quality_output_loss: 0.8541 - age_output_loss: 1.2786 - weight_output_loss: 0.8601 - bag_output_loss: 0.7145 - footwear_output_loss: 0.6264 - pose_output_loss: 0.2789 - emotion_output_loss: 0.8188 - gender_output_acc: 0.9402 - image_quality_output_acc: 0.6006 - age_output_acc: 0.4417 - weight_output_acc: 0.6558 - bag_output_acc: 0.7033 - footwear_output_acc: 0.7348 - pose_output_acc: 0.9034 - emotion_output_acc: 0.7153 - val_loss: 6.6506 - val_gender_output_loss: 0.3708 - val_image_quality_output_loss: 0.9081 - val_age_output_loss: 1.3978 - val_weight_output_loss: 0.9347 - val_bag_output_loss: 0.8374 - val_footwear_output_loss: 0.7672 - val_pose_output_loss: 0.5232 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.8513 - val_image_quality_output_acc: 0.5741 - val_age_output_acc: 0.3891 - val_weight_output_acc: 0.6499 - val_bag_output_acc: 0.6288 - val_footwear_output_acc: 0.6686 - val_pose_output_acc: 0.7969 - val_emotion_output_acc: 0.6850\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 6.61179\n",
            "Epoch 8/25\n",
            "625/625 [==============================] - 98s 157ms/step - loss: 5.6046 - gender_output_loss: 0.1674 - image_quality_output_loss: 0.8529 - age_output_loss: 1.2810 - weight_output_loss: 0.8598 - bag_output_loss: 0.7203 - footwear_output_loss: 0.6285 - pose_output_loss: 0.2807 - emotion_output_loss: 0.8140 - gender_output_acc: 0.9403 - image_quality_output_acc: 0.5999 - age_output_acc: 0.4433 - weight_output_acc: 0.6582 - bag_output_acc: 0.6979 - footwear_output_acc: 0.7339 - pose_output_acc: 0.9033 - emotion_output_acc: 0.7162 - val_loss: 6.7025 - val_gender_output_loss: 0.3869 - val_image_quality_output_loss: 0.9141 - val_age_output_loss: 1.4098 - val_weight_output_loss: 0.9205 - val_bag_output_loss: 0.8585 - val_footwear_output_loss: 0.7849 - val_pose_output_loss: 0.5328 - val_emotion_output_loss: 0.8951 - val_gender_output_acc: 0.8470 - val_image_quality_output_acc: 0.5706 - val_age_output_acc: 0.3818 - val_weight_output_acc: 0.6557 - val_bag_output_acc: 0.6177 - val_footwear_output_acc: 0.6656 - val_pose_output_acc: 0.7939 - val_emotion_output_acc: 0.6953\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.61179\n",
            "Epoch 9/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 5.5999 - gender_output_loss: 0.1663 - image_quality_output_loss: 0.8515 - age_output_loss: 1.2793 - weight_output_loss: 0.8629 - bag_output_loss: 0.7186 - footwear_output_loss: 0.6255 - pose_output_loss: 0.2816 - emotion_output_loss: 0.8142 - gender_output_acc: 0.9411 - image_quality_output_acc: 0.6010 - age_output_acc: 0.4434 - weight_output_acc: 0.6562 - bag_output_acc: 0.6986 - footwear_output_acc: 0.7387 - pose_output_acc: 0.9025 - emotion_output_acc: 0.7160 - val_loss: 6.7034 - val_gender_output_loss: 0.3709 - val_image_quality_output_loss: 0.9076 - val_age_output_loss: 1.4000 - val_weight_output_loss: 0.9512 - val_bag_output_loss: 0.8469 - val_footwear_output_loss: 0.7840 - val_pose_output_loss: 0.5135 - val_emotion_output_loss: 0.9294 - val_gender_output_acc: 0.8536 - val_image_quality_output_acc: 0.5769 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6434 - val_bag_output_acc: 0.6237 - val_footwear_output_acc: 0.6603 - val_pose_output_acc: 0.8057 - val_emotion_output_acc: 0.6774\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 6.61179\n",
            "Epoch 10/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 5.5690 - gender_output_loss: 0.1595 - image_quality_output_loss: 0.8503 - age_output_loss: 1.2768 - weight_output_loss: 0.8579 - bag_output_loss: 0.7089 - footwear_output_loss: 0.6259 - pose_output_loss: 0.2741 - emotion_output_loss: 0.8154 - gender_output_acc: 0.9441 - image_quality_output_acc: 0.5997 - age_output_acc: 0.4439 - weight_output_acc: 0.6562 - bag_output_acc: 0.7071 - footwear_output_acc: 0.7350 - pose_output_acc: 0.9082 - emotion_output_acc: 0.7152 - val_loss: 6.6574 - val_gender_output_loss: 0.3807 - val_image_quality_output_loss: 0.9094 - val_age_output_loss: 1.3962 - val_weight_output_loss: 0.9371 - val_bag_output_loss: 0.8379 - val_footwear_output_loss: 0.7730 - val_pose_output_loss: 0.5259 - val_emotion_output_loss: 0.8971 - val_gender_output_acc: 0.8485 - val_image_quality_output_acc: 0.5678 - val_age_output_acc: 0.3964 - val_weight_output_acc: 0.6520 - val_bag_output_acc: 0.6288 - val_footwear_output_acc: 0.6636 - val_pose_output_acc: 0.7901 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.61179\n",
            "Epoch 11/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 5.5703 - gender_output_loss: 0.1633 - image_quality_output_loss: 0.8529 - age_output_loss: 1.2762 - weight_output_loss: 0.8578 - bag_output_loss: 0.7120 - footwear_output_loss: 0.6232 - pose_output_loss: 0.2750 - emotion_output_loss: 0.8099 - gender_output_acc: 0.9415 - image_quality_output_acc: 0.6001 - age_output_acc: 0.4436 - weight_output_acc: 0.6578 - bag_output_acc: 0.7056 - footwear_output_acc: 0.7367 - pose_output_acc: 0.9045 - emotion_output_acc: 0.7179 - val_loss: 6.6819 - val_gender_output_loss: 0.3738 - val_image_quality_output_loss: 0.9087 - val_age_output_loss: 1.3999 - val_weight_output_loss: 0.9316 - val_bag_output_loss: 0.8528 - val_footwear_output_loss: 0.7774 - val_pose_output_loss: 0.5248 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.8538 - val_image_quality_output_acc: 0.5759 - val_age_output_acc: 0.3949 - val_weight_output_acc: 0.6537 - val_bag_output_acc: 0.6157 - val_footwear_output_acc: 0.6704 - val_pose_output_acc: 0.7979 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 6.61179\n",
            "Epoch 12/25\n",
            "625/625 [==============================] - 98s 157ms/step - loss: 5.5520 - gender_output_loss: 0.1579 - image_quality_output_loss: 0.8493 - age_output_loss: 1.2734 - weight_output_loss: 0.8565 - bag_output_loss: 0.7121 - footwear_output_loss: 0.6177 - pose_output_loss: 0.2721 - emotion_output_loss: 0.8129 - gender_output_acc: 0.9437 - image_quality_output_acc: 0.6023 - age_output_acc: 0.4452 - weight_output_acc: 0.6564 - bag_output_acc: 0.7048 - footwear_output_acc: 0.7381 - pose_output_acc: 0.9089 - emotion_output_acc: 0.7160 - val_loss: 6.6988 - val_gender_output_loss: 0.3901 - val_image_quality_output_loss: 0.9076 - val_age_output_loss: 1.3966 - val_weight_output_loss: 0.9378 - val_bag_output_loss: 0.8402 - val_footwear_output_loss: 0.7840 - val_pose_output_loss: 0.5308 - val_emotion_output_loss: 0.9118 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.5761 - val_age_output_acc: 0.3901 - val_weight_output_acc: 0.6454 - val_bag_output_acc: 0.6265 - val_footwear_output_acc: 0.6631 - val_pose_output_acc: 0.7971 - val_emotion_output_acc: 0.6862\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.61179\n",
            "Epoch 13/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 5.5540 - gender_output_loss: 0.1572 - image_quality_output_loss: 0.8514 - age_output_loss: 1.2717 - weight_output_loss: 0.8531 - bag_output_loss: 0.7109 - footwear_output_loss: 0.6229 - pose_output_loss: 0.2745 - emotion_output_loss: 0.8123 - gender_output_acc: 0.9435 - image_quality_output_acc: 0.5998 - age_output_acc: 0.4407 - weight_output_acc: 0.6612 - bag_output_acc: 0.7040 - footwear_output_acc: 0.7369 - pose_output_acc: 0.9065 - emotion_output_acc: 0.7171 - val_loss: 6.6928 - val_gender_output_loss: 0.3750 - val_image_quality_output_loss: 0.9018 - val_age_output_loss: 1.4052 - val_weight_output_loss: 0.9410 - val_bag_output_loss: 0.8553 - val_footwear_output_loss: 0.7766 - val_pose_output_loss: 0.5314 - val_emotion_output_loss: 0.9063 - val_gender_output_acc: 0.8533 - val_image_quality_output_acc: 0.5771 - val_age_output_acc: 0.3833 - val_weight_output_acc: 0.6434 - val_bag_output_acc: 0.6157 - val_footwear_output_acc: 0.6709 - val_pose_output_acc: 0.8007 - val_emotion_output_acc: 0.6903\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 6.61179\n",
            "Epoch 14/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 5.5534 - gender_output_loss: 0.1596 - image_quality_output_loss: 0.8450 - age_output_loss: 1.2740 - weight_output_loss: 0.8553 - bag_output_loss: 0.7126 - footwear_output_loss: 0.6239 - pose_output_loss: 0.2715 - emotion_output_loss: 0.8115 - gender_output_acc: 0.9442 - image_quality_output_acc: 0.6013 - age_output_acc: 0.4468 - weight_output_acc: 0.6546 - bag_output_acc: 0.7049 - footwear_output_acc: 0.7358 - pose_output_acc: 0.9066 - emotion_output_acc: 0.7164 - val_loss: 6.6975 - val_gender_output_loss: 0.3841 - val_image_quality_output_loss: 0.9075 - val_age_output_loss: 1.3906 - val_weight_output_loss: 0.9401 - val_bag_output_loss: 0.8449 - val_footwear_output_loss: 0.7908 - val_pose_output_loss: 0.5218 - val_emotion_output_loss: 0.9176 - val_gender_output_acc: 0.8463 - val_image_quality_output_acc: 0.5741 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6469 - val_bag_output_acc: 0.6232 - val_footwear_output_acc: 0.6595 - val_pose_output_acc: 0.7984 - val_emotion_output_acc: 0.6830\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 6.61179\n",
            "Epoch 15/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 5.5488 - gender_output_loss: 0.1539 - image_quality_output_loss: 0.8494 - age_output_loss: 1.2785 - weight_output_loss: 0.8553 - bag_output_loss: 0.7112 - footwear_output_loss: 0.6176 - pose_output_loss: 0.2668 - emotion_output_loss: 0.8161 - gender_output_acc: 0.9448 - image_quality_output_acc: 0.6008 - age_output_acc: 0.4454 - weight_output_acc: 0.6605 - bag_output_acc: 0.7054 - footwear_output_acc: 0.7402 - pose_output_acc: 0.9095 - emotion_output_acc: 0.7145 - val_loss: 6.7192 - val_gender_output_loss: 0.3816 - val_image_quality_output_loss: 0.9108 - val_age_output_loss: 1.3986 - val_weight_output_loss: 0.9444 - val_bag_output_loss: 0.8551 - val_footwear_output_loss: 0.7873 - val_pose_output_loss: 0.5295 - val_emotion_output_loss: 0.9120 - val_gender_output_acc: 0.8488 - val_image_quality_output_acc: 0.5759 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6195 - val_footwear_output_acc: 0.6626 - val_pose_output_acc: 0.7954 - val_emotion_output_acc: 0.6865\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 6.61179\n",
            "Epoch 16/25\n",
            "624/625 [============================>.] - ETA: 0s - loss: 5.5312 - gender_output_loss: 0.1555 - image_quality_output_loss: 0.8478 - age_output_loss: 1.2720 - weight_output_loss: 0.8538 - bag_output_loss: 0.7082 - footwear_output_loss: 0.6181 - pose_output_loss: 0.2670 - emotion_output_loss: 0.8087 - gender_output_acc: 0.9456 - image_quality_output_acc: 0.6026 - age_output_acc: 0.4445 - weight_output_acc: 0.6582 - bag_output_acc: 0.7036 - footwear_output_acc: 0.7388 - pose_output_acc: 0.9090 - emotion_output_acc: 0.7177\n",
            "Epoch 00015: val_loss did not improve from 6.61179\n",
            "Epoch 16/25\n",
            "625/625 [==============================] - 100s 159ms/step - loss: 5.5296 - gender_output_loss: 0.1555 - image_quality_output_loss: 0.8476 - age_output_loss: 1.2717 - weight_output_loss: 0.8537 - bag_output_loss: 0.7082 - footwear_output_loss: 0.6178 - pose_output_loss: 0.2670 - emotion_output_loss: 0.8081 - gender_output_acc: 0.9456 - image_quality_output_acc: 0.6028 - age_output_acc: 0.4445 - weight_output_acc: 0.6583 - bag_output_acc: 0.7036 - footwear_output_acc: 0.7391 - pose_output_acc: 0.9090 - emotion_output_acc: 0.7180 - val_loss: 6.7115 - val_gender_output_loss: 0.3780 - val_image_quality_output_loss: 0.9043 - val_age_output_loss: 1.4021 - val_weight_output_loss: 0.9439 - val_bag_output_loss: 0.8527 - val_footwear_output_loss: 0.7884 - val_pose_output_loss: 0.5330 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.8508 - val_image_quality_output_acc: 0.5786 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6464 - val_bag_output_acc: 0.6152 - val_footwear_output_acc: 0.6620 - val_pose_output_acc: 0.7979 - val_emotion_output_acc: 0.6872\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 6.61179\n",
            "Epoch 17/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 5.5384 - gender_output_loss: 0.1521 - image_quality_output_loss: 0.8465 - age_output_loss: 1.2736 - weight_output_loss: 0.8523 - bag_output_loss: 0.7090 - footwear_output_loss: 0.6176 - pose_output_loss: 0.2677 - emotion_output_loss: 0.8196 - gender_output_acc: 0.9467 - image_quality_output_acc: 0.6014 - age_output_acc: 0.4432 - weight_output_acc: 0.6578 - bag_output_acc: 0.7074 - footwear_output_acc: 0.7386 - pose_output_acc: 0.9090 - emotion_output_acc: 0.7131 - val_loss: 6.6989 - val_gender_output_loss: 0.3837 - val_image_quality_output_loss: 0.9068 - val_age_output_loss: 1.3988 - val_weight_output_loss: 0.9347 - val_bag_output_loss: 0.8556 - val_footwear_output_loss: 0.7861 - val_pose_output_loss: 0.5221 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.8460 - val_image_quality_output_acc: 0.5761 - val_age_output_acc: 0.3921 - val_weight_output_acc: 0.6482 - val_bag_output_acc: 0.6174 - val_footwear_output_acc: 0.6676 - val_pose_output_acc: 0.7996 - val_emotion_output_acc: 0.6870\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 6.61179\n",
            "Epoch 18/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 5.5263 - gender_output_loss: 0.1549 - image_quality_output_loss: 0.8492 - age_output_loss: 1.2696 - weight_output_loss: 0.8544 - bag_output_loss: 0.7065 - footwear_output_loss: 0.6172 - pose_output_loss: 0.2668 - emotion_output_loss: 0.8078 - gender_output_acc: 0.9447 - image_quality_output_acc: 0.6021 - age_output_acc: 0.4456 - weight_output_acc: 0.6616 - bag_output_acc: 0.7071 - footwear_output_acc: 0.7407 - pose_output_acc: 0.9096 - emotion_output_acc: 0.7172 - val_loss: 6.7429 - val_gender_output_loss: 0.3908 - val_image_quality_output_loss: 0.9140 - val_age_output_loss: 1.4042 - val_weight_output_loss: 0.9475 - val_bag_output_loss: 0.8520 - val_footwear_output_loss: 0.7862 - val_pose_output_loss: 0.5351 - val_emotion_output_loss: 0.9131 - val_gender_output_acc: 0.8470 - val_image_quality_output_acc: 0.5688 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6177 - val_footwear_output_acc: 0.6636 - val_pose_output_acc: 0.7966 - val_emotion_output_acc: 0.6850\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 6.61179\n",
            "Epoch 19/25\n",
            "625/625 [==============================] - 98s 157ms/step - loss: 5.5229 - gender_output_loss: 0.1516 - image_quality_output_loss: 0.8458 - age_output_loss: 1.2705 - weight_output_loss: 0.8514 - bag_output_loss: 0.7102 - footwear_output_loss: 0.6187 - pose_output_loss: 0.2649 - emotion_output_loss: 0.8098 - gender_output_acc: 0.9462 - image_quality_output_acc: 0.6067 - age_output_acc: 0.4466 - weight_output_acc: 0.6605 - bag_output_acc: 0.7058 - footwear_output_acc: 0.7376 - pose_output_acc: 0.9090 - emotion_output_acc: 0.7166 - val_loss: 6.7185 - val_gender_output_loss: 0.3932 - val_image_quality_output_loss: 0.9127 - val_age_output_loss: 1.3936 - val_weight_output_loss: 0.9246 - val_bag_output_loss: 0.8583 - val_footwear_output_loss: 0.7820 - val_pose_output_loss: 0.5324 - val_emotion_output_loss: 0.9217 - val_gender_output_acc: 0.8473 - val_image_quality_output_acc: 0.5706 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6510 - val_bag_output_acc: 0.6159 - val_footwear_output_acc: 0.6694 - val_pose_output_acc: 0.7961 - val_emotion_output_acc: 0.6812\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 6.61179\n",
            "Epoch 20/25\n",
            "625/625 [==============================] - 99s 158ms/step - loss: 5.5164 - gender_output_loss: 0.1516 - image_quality_output_loss: 0.8477 - age_output_loss: 1.2685 - weight_output_loss: 0.8493 - bag_output_loss: 0.7068 - footwear_output_loss: 0.6152 - pose_output_loss: 0.2635 - emotion_output_loss: 0.8138 - gender_output_acc: 0.9461 - image_quality_output_acc: 0.6041 - age_output_acc: 0.4498 - weight_output_acc: 0.6588 - bag_output_acc: 0.7077 - footwear_output_acc: 0.7383 - pose_output_acc: 0.9109 - emotion_output_acc: 0.7147 - val_loss: 6.7316 - val_gender_output_loss: 0.3863 - val_image_quality_output_loss: 0.9124 - val_age_output_loss: 1.4174 - val_weight_output_loss: 0.9361 - val_bag_output_loss: 0.8423 - val_footwear_output_loss: 0.7886 - val_pose_output_loss: 0.5383 - val_emotion_output_loss: 0.9101 - val_gender_output_acc: 0.8478 - val_image_quality_output_acc: 0.5721 - val_age_output_acc: 0.3841 - val_weight_output_acc: 0.6464 - val_bag_output_acc: 0.6230 - val_footwear_output_acc: 0.6658 - val_pose_output_acc: 0.8007 - val_emotion_output_acc: 0.6913\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 6.61179\n",
            "Epoch 21/25\n",
            "625/625 [==============================] - 100s 159ms/step - loss: 5.5204 - gender_output_loss: 0.1518 - image_quality_output_loss: 0.8489 - age_output_loss: 1.2681 - weight_output_loss: 0.8496 - bag_output_loss: 0.7078 - footwear_output_loss: 0.6205 - pose_output_loss: 0.2644 - emotion_output_loss: 0.8093 - gender_output_acc: 0.9458 - image_quality_output_acc: 0.6019 - age_output_acc: 0.4476 - weight_output_acc: 0.6608 - bag_output_acc: 0.7093 - footwear_output_acc: 0.7394 - pose_output_acc: 0.9099 - emotion_output_acc: 0.7168 - val_loss: 6.7170 - val_gender_output_loss: 0.3885 - val_image_quality_output_loss: 0.9100 - val_age_output_loss: 1.3929 - val_weight_output_loss: 0.9454 - val_bag_output_loss: 0.8511 - val_footwear_output_loss: 0.7950 - val_pose_output_loss: 0.5293 - val_emotion_output_loss: 0.9047 - val_gender_output_acc: 0.8523 - val_image_quality_output_acc: 0.5698 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6462 - val_bag_output_acc: 0.6174 - val_footwear_output_acc: 0.6643 - val_pose_output_acc: 0.7986 - val_emotion_output_acc: 0.6890\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 6.61179\n",
            "Epoch 22/25\n",
            "625/625 [==============================] - 101s 161ms/step - loss: 5.5044 - gender_output_loss: 0.1526 - image_quality_output_loss: 0.8433 - age_output_loss: 1.2703 - weight_output_loss: 0.8512 - bag_output_loss: 0.7049 - footwear_output_loss: 0.6115 - pose_output_loss: 0.2630 - emotion_output_loss: 0.8076 - gender_output_acc: 0.9453 - image_quality_output_acc: 0.6057 - age_output_acc: 0.4475 - weight_output_acc: 0.6593 - bag_output_acc: 0.7079 - footwear_output_acc: 0.7437 - pose_output_acc: 0.9103 - emotion_output_acc: 0.7179 - val_loss: 6.7195 - val_gender_output_loss: 0.3864 - val_image_quality_output_loss: 0.9102 - val_age_output_loss: 1.3966 - val_weight_output_loss: 0.9326 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.7843 - val_pose_output_loss: 0.5252 - val_emotion_output_loss: 0.9222 - val_gender_output_acc: 0.8501 - val_image_quality_output_acc: 0.5658 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6449 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.6636 - val_pose_output_acc: 0.7974 - val_emotion_output_acc: 0.6827\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 6.61179\n",
            "Epoch 23/25\n",
            "625/625 [==============================] - 100s 160ms/step - loss: 5.5123 - gender_output_loss: 0.1496 - image_quality_output_loss: 0.8466 - age_output_loss: 1.2715 - weight_output_loss: 0.8506 - bag_output_loss: 0.7056 - footwear_output_loss: 0.6140 - pose_output_loss: 0.2616 - emotion_output_loss: 0.8128 - gender_output_acc: 0.9467 - image_quality_output_acc: 0.6044 - age_output_acc: 0.4470 - weight_output_acc: 0.6593 - bag_output_acc: 0.7088 - footwear_output_acc: 0.7388 - pose_output_acc: 0.9099 - emotion_output_acc: 0.7151 - val_loss: 6.7583 - val_gender_output_loss: 0.3885 - val_image_quality_output_loss: 0.9109 - val_age_output_loss: 1.4048 - val_weight_output_loss: 0.9428 - val_bag_output_loss: 0.8567 - val_footwear_output_loss: 0.7916 - val_pose_output_loss: 0.5476 - val_emotion_output_loss: 0.9153 - val_gender_output_acc: 0.8546 - val_image_quality_output_acc: 0.5781 - val_age_output_acc: 0.3921 - val_weight_output_acc: 0.6424 - val_bag_output_acc: 0.6139 - val_footwear_output_acc: 0.6552 - val_pose_output_acc: 0.7949 - val_emotion_output_acc: 0.6825\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 6.61179\n",
            "Epoch 24/25\n",
            "625/625 [==============================] - 100s 159ms/step - loss: 5.5062 - gender_output_loss: 0.1483 - image_quality_output_loss: 0.8455 - age_output_loss: 1.2690 - weight_output_loss: 0.8503 - bag_output_loss: 0.7060 - footwear_output_loss: 0.6141 - pose_output_loss: 0.2629 - emotion_output_loss: 0.8101 - gender_output_acc: 0.9487 - image_quality_output_acc: 0.6035 - age_output_acc: 0.4474 - weight_output_acc: 0.6591 - bag_output_acc: 0.7087 - footwear_output_acc: 0.7395 - pose_output_acc: 0.9106 - emotion_output_acc: 0.7168 - val_loss: 6.7576 - val_gender_output_loss: 0.4001 - val_image_quality_output_loss: 0.9145 - val_age_output_loss: 1.4125 - val_weight_output_loss: 0.9319 - val_bag_output_loss: 0.8509 - val_footwear_output_loss: 0.7892 - val_pose_output_loss: 0.5520 - val_emotion_output_loss: 0.9064 - val_gender_output_acc: 0.8430 - val_image_quality_output_acc: 0.5668 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6484 - val_bag_output_acc: 0.6200 - val_footwear_output_acc: 0.6628 - val_pose_output_acc: 0.7921 - val_emotion_output_acc: 0.6905\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 6.61179\n",
            "Epoch 25/25\n",
            "625/625 [==============================] - 99s 159ms/step - loss: 5.5077 - gender_output_loss: 0.1499 - image_quality_output_loss: 0.8448 - age_output_loss: 1.2709 - weight_output_loss: 0.8518 - bag_output_loss: 0.7069 - footwear_output_loss: 0.6143 - pose_output_loss: 0.2587 - emotion_output_loss: 0.8104 - gender_output_acc: 0.9456 - image_quality_output_acc: 0.6048 - age_output_acc: 0.4477 - weight_output_acc: 0.6596 - bag_output_acc: 0.7074 - footwear_output_acc: 0.7416 - pose_output_acc: 0.9112 - emotion_output_acc: 0.7166 - val_loss: 6.7331 - val_gender_output_loss: 0.3835 - val_image_quality_output_loss: 0.9129 - val_age_output_loss: 1.3971 - val_weight_output_loss: 0.9448 - val_bag_output_loss: 0.8519 - val_footwear_output_loss: 0.7830 - val_pose_output_loss: 0.5369 - val_emotion_output_loss: 0.9231 - val_gender_output_acc: 0.8493 - val_image_quality_output_acc: 0.5736 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.6159 - val_footwear_output_acc: 0.6696 - val_pose_output_acc: 0.7979 - val_emotion_output_acc: 0.6827\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 6.61179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGP01E-Ntpjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}